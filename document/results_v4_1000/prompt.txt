/Users/takumi0616/Develop/docker_miniconda/src/PressurePattern/main_v4.py
```
# main_v4.py
# -*- coding: utf-8 -*-
import os
import sys
import json
import logging
import time
from typing import Optional, List, Dict, Tuple
from collections import Counter, defaultdict

import numpy as np
import pandas as pd
import xarray as xr
import torch
import torch.nn.functional as F

import matplotlib
matplotlib.use('Agg')  # サーバ上でも保存できるように
import matplotlib.pyplot as plt

# cartopy: 可視化に使用
import cartopy.crs as ccrs
import cartopy.feature as cfeature
from matplotlib.colors import Normalize

# 3type版 SOM（Euclidean/SSIM/S1対応, batchSOM）
from minisom import MiniSom as MultiDistMiniSom

# =====================================================
# ユーザ調整パラメータ
# =====================================================
SEED = 1

# SOM学習・推論（全期間版：3方式）
SOM_X, SOM_Y = 10, 10
NUM_ITER = 1000
BATCH_SIZE = 128 
NODES_CHUNK = 4 # VRAM16GB:2, VRAM24GB:4
LOG_INTERVAL = 10
EVAL_SAMPLE_LIMIT = 4000
SOM_EVAL_SEGMENTS = 100  # NUM_ITER をこの個数の区間に分割して評価（100区切り）

# データ
DATA_FILE = './prmsl_era5_all_data_seasonal_large.nc'

# 期間（学習/検証）
LEARN_START = '1991-01-01'
LEARN_END   = '1999-12-31'
VALID_START = '2000-01-01'
VALID_END   = '2000-12-31'

# 出力先（v4）
RESULT_DIR   = './results_v4'
LEARNING_ROOT = os.path.join(RESULT_DIR, 'learning_result')
VERIF_ROOT    = os.path.join(RESULT_DIR, 'verification_results')

# 基本ラベル（15）
BASE_LABELS = [
    '1', '2A', '2B', '2C', '2D', '3A', '3B', '3C', '3D', '4A', '4B', '5', '6A', '6B', '6C'
]

# SSIMの定数（ミニソムに合わせる）
SSIM_C1 = 1e-8
SSIM_C2 = 1e-8

# =====================================================
# 再現性・ログ
# =====================================================
def set_reproducibility(seed: int = 42):
    os.environ['PYTHONHASHSEED'] = str(seed)
    os.environ['OPENBLAS_NUM_THREADS'] = '1'
    os.environ['MKL_NUM_THREADS'] = '1'
    os.environ['OMP_NUM_THREADS'] = '1'
    os.environ['NUMEXPR_NUM_THREADS'] = '1'
    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'
    import random
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    try:
        torch.use_deterministic_algorithms(True)
    except Exception:
        pass
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    if torch.cuda.is_available():
        torch.backends.cuda.matmul.allow_tf32 = False
        torch.backends.cudnn.allow_tf32 = False
    try:
        torch.set_num_threads(1)
    except Exception:
        pass


def setup_logging_v4():
    for d in [RESULT_DIR, LEARNING_ROOT, VERIF_ROOT]:
        os.makedirs(d, exist_ok=True)
    log_path = os.path.join(RESULT_DIR, 'run_v4.log')
    if os.path.exists(log_path):
        os.remove(log_path)
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[logging.FileHandler(log_path), logging.StreamHandler()]
    )
    logging.info("ログ初期化完了（run_v4.log）。")


# =====================================================
# 共通ユーティリティ
# =====================================================
def format_date_yyyymmdd(ts_val) -> str:
    """
    ndarrayのdatetime64や文字列混在に対しても YYYY/MM/DD に整形。
    失敗時は str(ts_val) を返す。
    """
    if ts_val is None:
        return ''
    try:
        ts = pd.to_datetime(ts_val)
        if pd.isna(ts):
            return ''
        return ts.strftime('%Y/%m/%d')
    except Exception:
        try:
            s = str(ts_val)
            if 'T' in s:
                s = s.split('T')[0]
            s = s.replace('-', '/')
            if len(s) >= 10:
                return s[:10]
            return s
        except Exception:
            return str(ts_val)


# =====================================================
# データ読み込み ＆ 前処理（hPa偏差、空間平均差し引き）
# =====================================================
def load_and_prepare_data_unified(filepath: str,
                                  start_date: Optional[str],
                                  end_date: Optional[str],
                                  device: str = 'cpu'):
    logging.info(f"データ読み込み: {filepath}")
    ds = xr.open_dataset(filepath, decode_times=True)

    # time座標名の検出
    if 'valid_time' in ds:
        time_coord = 'valid_time'
    elif 'time' in ds:
        time_coord = 'time'
    else:
        raise ValueError('No time coordinate named "valid_time" or "time".')

    # 期間指定
    if (start_date is not None) or (end_date is not None):
        sub = ds.sel({time_coord: slice(start_date, end_date)})
    else:
        sub = ds

    if 'msl' not in sub:
        raise ValueError('Variable "msl" not found in dataset.')

    msl = sub['msl'].astype('float32')

    # 次元名の標準化
    lat_name = 'latitude'
    lon_name = 'longitude'
    for dn in msl.dims:
        if 'lat' in dn.lower(): lat_name = dn
        if 'lon' in dn.lower(): lon_name = dn

    msl = msl.transpose(time_coord, lat_name, lon_name)  # (N,H,W)
    ntime = msl.sizes[time_coord]
    nlat = msl.sizes[lat_name]
    nlon = msl.sizes[lon_name]

    arr = msl.values  # (N,H,W) in Pa
    arr2 = arr.reshape(ntime, nlat*nlon)  # (N,D)

    # NaN行除外
    valid_mask = ~np.isnan(arr2).any(axis=1)
    arr2 = arr2[valid_mask]
    times = msl[time_coord].values[valid_mask]
    lat = sub[lat_name].values
    lon = sub[lon_name].values

    # ラベル（あれば）
    labels = None
    if 'label' in sub.variables:
        raw = sub['label'].values
        raw = raw[valid_mask]
        labels = [v.decode('utf-8') if isinstance(v, (bytes, bytearray)) else str(v) for v in raw]
        logging.info("ラベルを読み込みました。")

    # hPaへ換算 → 空間平均差し引き
    msl_hpa_flat = (arr2 / 100.0).astype(np.float32)  # (N,D)
    mean_per_sample = np.nanmean(msl_hpa_flat, axis=1, keepdims=True)
    anomaly_flat = msl_hpa_flat - mean_per_sample  # (N,D)
    X_for_s1 = torch.from_numpy(anomaly_flat).to(device=device, dtype=torch.float32)

    # 3D形状に戻す
    n = anomaly_flat.shape[0]
    msl_hpa = msl_hpa_flat.reshape(n, nlat, nlon)
    anomaly_hpa = anomaly_flat.reshape(n, nlat, nlon)

    logging.info(f"期間: {str(times.min()) if len(times)>0 else '?'} 〜 {str(times.max()) if len(times)>0 else '?'}")
    logging.info(f"サンプル数={n}, 解像度={nlat}x{nlon}")
    return X_for_s1, msl_hpa, anomaly_hpa, lat, lon, nlat, nlon, times, labels


# =====================================================
# 評価ユーティリティ（s1_clustering.pyから必要部分を移植）
# =====================================================
def _normalize_to_base_candidate(label_str: Optional[str]) -> Optional[str]:
    import unicodedata, re
    if label_str is None:
        return None
    s = str(label_str)
    s = unicodedata.normalize('NFKC', s)
    s = s.upper().strip()
    s = s.replace('＋', '+').replace('－', '-').replace('−', '-')
    s = re.sub(r'[^0-9A-Z]', '', s)
    return s if s != '' else None


def basic_label_or_none(label_str: Optional[str], base_labels: List[str]) -> Optional[str]:
    import re
    cand = _normalize_to_base_candidate(label_str)
    if cand is None:
        return None
    # 完全一致を優先
    if cand in base_labels:
        return cand
    # '2A+' → '2A' のようなパターンを許容（残りに英数字がなければOK）
    for bl in base_labels:
        if cand == bl:
            return bl
        if cand.startswith(bl):
            rest = cand[len(bl):]
            if re.search(r'[0-9A-Z]', rest) is None:
                return bl
    return None


def extract_base_components(raw_label: Optional[str], base_labels: List[str]) -> List[str]:
    import unicodedata, re
    if raw_label is None:
        return []
    s = unicodedata.normalize('NFKC', str(raw_label)).upper().strip()
    s = s.replace('＋', '+').replace('－', '-').replace('−', '-')
    tokens = re.split(r'[^0-9A-Z]+', s)
    comps: List[str] = []
    for t in tokens:
        if t in base_labels and t not in comps:
            comps.append(t)
    return comps


def primary_base_label(raw_label: Optional[str], base_labels: List[str]) -> Optional[str]:
    parts = extract_base_components(raw_label, base_labels)
    return parts[0] if parts else None


def build_confusion_matrix_only_base(clusters: List[List[int]],
                                     all_labels: List[Optional[str]],
                                     base_labels: List[str]) -> Tuple[pd.DataFrame, List[str]]:
    num_clusters = len(clusters)
    cluster_names = [f'Cluster_{i+1}' for i in range(num_clusters)]
    cm = pd.DataFrame(0, index=base_labels, columns=cluster_names, dtype=int)
    for i, idxs in enumerate(clusters):
        col = cluster_names[i]
        cnt = Counter()
        for j in idxs:
            lbl = basic_label_or_none(all_labels[j], base_labels)
            if lbl is not None:
                cnt[lbl] += 1
        for lbl, k in cnt.items():
            cm.loc[lbl, col] = k
    return cm, cluster_names


def evaluate_clusters_only_base(clusters: List[List[int]],
                                all_labels: List[Optional[str]],
                                base_labels: List[str],
                                title: str = "評価（基本ラベルのみ）") -> Optional[Dict[str, float]]:
    logging.info(f"\n--- {title} ---")
    if not all_labels:
        logging.warning("ラベル無しのため評価をスキップします。")
        return None

    cm, cluster_names = build_confusion_matrix_only_base(clusters, all_labels, base_labels)
    present_labels = [l for l in base_labels if cm.loc[l].sum() > 0]
    if len(present_labels) == 0:
        logging.warning("基本ラベルに該当するサンプルがありません。評価をスキップします。")
        return None

    logging.info("【混同行列（基本ラベルのみ）】\n" + "\n" + cm.loc[present_labels, :].to_string())

    # 各クラスタの多数決（代表ラベル）
    cluster_majority: Dict[int, Optional[str]] = {}
    logging.info("\n【各クラスタの多数決（代表ラベル）】")
    for k in range(len(cluster_names)):
        col = cluster_names[k]
        col_counts = cm[col]
        col_sum = int(col_counts.sum())
        if col_sum == 0:
            cluster_majority[k] = None
            logging.info(f" - {col:<12}: 代表ラベル=None（基本ラベル出現なし）")
            continue
        top_label = col_counts.idxmax()
        top_count = int(col_counts.max())
        share = top_count / col_sum if col_sum > 0 else 0.0
        top3 = col_counts.sort_values(ascending=False)[:3]
        top3_str = ", ".join([f"{lbl}:{int(cnt)}" for lbl, cnt in top3.items()])
        logging.info(f" - {col:<12}: 代表={top_label:<3} 件数={top_count:4d} シェア={share:5.2f} | 上位: {top3_str}")
        cluster_majority[k] = top_label

    # Macro Recall (基本ラベル)
    logging.info("\n【各ラベルの再現率（代表クラスタ群ベース）】")
    per_label = {}
    for lbl in present_labels:
        row_sum = int(cm.loc[lbl, :].sum())
        cols_for_lbl = [cluster_names[k] for k in range(len(cluster_names)) if cluster_majority.get(k, None) == lbl]
        correct = int(cm.loc[lbl, cols_for_lbl].sum()) if cols_for_lbl else 0
        recall = correct / row_sum if row_sum > 0 else 0.0
        per_label[lbl] = {'N': row_sum, 'Correct': correct, 'Recall': recall}
        logging.info(f" - {lbl:<3}: N={row_sum:4d} Correct={correct:4d} Recall={recall:.4f} 代表={cols_for_lbl if cols_for_lbl else 'なし'}")
    macro_recall = float(np.mean([per_label[l]['Recall'] for l in present_labels]))

    # 複合ラベル考慮（基本+応用）
    logging.info("\n【複合ラベル考慮の再現率（基本+応用）】")
    composite_totals = Counter()
    for j, raw_label in enumerate(all_labels):
        components = extract_base_components(raw_label, base_labels)
        for comp in components:
            composite_totals[comp] += 1
    present_labels_composite = sorted([l for l in base_labels if composite_totals[l] > 0])

    macro_recall_composite = np.nan
    if present_labels_composite:
        composite_correct_recall = Counter()
        # 各サンプルの予測＝割当クラスタの代表（基本ラベル）
        # 代表が複合ラベルの構成に含まれていれば正解
        n_samples = sum(len(idxs) for idxs in clusters)
        sample_to_cluster = {}
        for ci, idxs in enumerate(clusters):
            for j in idxs:
                sample_to_cluster[j] = ci

        for j, raw_label in enumerate(all_labels):
            comps = extract_base_components(raw_label, base_labels)
            if not comps:
                continue
            ci = sample_to_cluster.get(j, -1)
            if ci < 0:
                continue
            pred = cluster_majority.get(ci)
            if pred is None:
                continue
            if pred in comps:
                composite_correct_recall[pred] += 1

        recalls_composite = []
        for lbl in present_labels_composite:
            total = int(composite_totals[lbl])
            correct = int(composite_correct_recall[lbl])
            recall = correct / total if total > 0 else 0.0
            recalls_composite.append(recall)
            logging.info(f" - {lbl:<3}: N={total:4d} Correct={correct:4d} Recall={recall:.4f}")
        if recalls_composite:
            macro_recall_composite = float(np.mean(recalls_composite))
    else:
        logging.warning("複合ラベル考慮での評価対象ラベルがありません.")

    metrics: Dict[str, float] = {
        'MacroRecall_majority': macro_recall,
        'MacroRecall_composite': macro_recall_composite
    }

    logging.info("\n【集計】")
    logging.info(f"Macro Recall (基本ラベル) = {macro_recall:.4f}")
    logging.info(f"Macro Recall (基本+応用) = {macro_recall_composite:.4f}")
    logging.info(f"--- {title} 終了 ---\n")
    return metrics


def plot_iteration_metrics(history: Dict[str, List[float]], save_path: str) -> None:
    iters = history.get('iteration', [])
    metrics_names = [k for k in history.keys() if k != 'iteration']
    n = len(metrics_names)
    n_cols = 2
    n_rows = (n + n_cols - 1) // n_cols if n > 0 else 1
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(6 * n_cols, 4 * n_rows))
    axes = np.atleast_1d(axes).flatten()
    for idx, mname in enumerate(metrics_names):
        ax = axes[idx]
        ax.plot(iters, history.get(mname, []), marker='o')
        ax.set_title(mname)
        ax.set_xlabel('Iteration')
        ax.set_ylabel(mname)
        ax.grid(True)
    for i in range(n, len(axes)):
        axes[i].axis("off")
    plt.tight_layout()
    plt.savefig(save_path)
    plt.close()


def plot_iteration_metrics_single(history: Dict[str, List[float]], out_dir: str, filename_prefix: str) -> None:
    """
    各指標ごとに1枚の画像を保存する（従来の4指標まとめ画像に加えて出力）
    """
    os.makedirs(out_dir, exist_ok=True)
    iters = history.get('iteration', [])
    for mname, values in history.items():
        if mname == 'iteration':
            continue
        fig = plt.figure(figsize=(6, 4))
        ax = plt.gca()
        ax.plot(iters, values, marker='o')
        ax.set_title(mname)
        ax.set_xlabel('Iteration')
        ax.set_ylabel(mname)
        ax.grid(True)
        fpath = os.path.join(out_dir, f'{filename_prefix}_iteration_vs_{mname}.png')
        plt.tight_layout()
        plt.savefig(fpath, dpi=200)
        plt.close(fig)


def save_metrics_history_to_csv(history: Dict[str, List[float]], out_csv: str) -> None:
    df = pd.DataFrame(history)
    df.to_csv(out_csv, index=False)


# =====================================================
# 3type_som側のユーティリティ（ログ・評価・可視化）
# =====================================================
class Logger:
    def __init__(self, path):
        self.path = path
        self.f = open(path, 'w', encoding='utf-8')
    def write(self, s):
        sys.stdout.write(s)
        self.f.write(s)
        self.f.flush()
    def close(self):
        self.f.close()


def winners_to_clusters(winners_xy, som_shape):
    clusters = [[] for _ in range(som_shape[0]*som_shape[1])]
    for i,(ix,iy) in enumerate(winners_xy):
        k = ix*som_shape[1] + iy
        clusters[k].append(i)
    return clusters


def plot_som_node_average_patterns(data_flat, winners_xy, lat, lon, som_shape, save_path, title):
    """
    ノード平均（セントロイド）マップ（偏差[hPa]）
    """
    H, W = len(lat), len(lon)
    X2 = data_flat.reshape(-1, H, W)  # 偏差[hPa]

    map_x, map_y = som_shape
    mean_patterns = np.full((map_x, map_y, H, W), np.nan, dtype=np.float32)
    counts = np.zeros((map_x, map_y), dtype=int)
    for ix in range(map_x):
        for iy in range(map_y):
            mask = (winners_xy[:,0]==ix) & (winners_xy[:,1]==iy)
            idxs = np.where(mask)[0]
            counts[ix,iy] = len(idxs)
            if len(idxs)>0:
                mean_patterns[ix,iy] = np.nanmean(X2[idxs], axis=0)

    vmin, vmax = -40, 40
    levels = np.linspace(vmin, vmax, 21)
    cmap = 'RdBu_r'

    nrows, ncols = som_shape[1], som_shape[0]
    figsize=(ncols*2.6, nrows*2.6)
    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=figsize,
                               subplot_kw={'projection': ccrs.PlateCarree()})
    axes = np.atleast_2d(axes)
    axes = axes.T[::-1,:]

    last_cf=None
    for ix in range(map_x):
        for iy in range(map_y):
            ax = axes[ix,iy]
            mp = mean_patterns[ix,iy]
            if np.isnan(mp).all():
                ax.set_axis_off(); continue
            cf = ax.contourf(lon, lat, mp, levels=levels, cmap=cmap, extend='both', transform=ccrs.PlateCarree())
            ax.contour(lon, lat, mp, levels=levels, colors='k', linewidths=0.3, transform=ccrs.PlateCarree())
            ax.add_feature(cfeature.COASTLINE.with_scale('50m'), edgecolor='black', linewidth=0.8)
            ax.set_extent([115, 155, 15, 55], ccrs.PlateCarree())
            ax.text(0.02,0.96,f'({ix},{iy}) N={counts[ix,iy]}', transform=ax.transAxes,
                    fontsize=7, va='top', bbox=dict(facecolor='white', alpha=0.7, edgecolor='none'))
            ax.set_xticks([]); ax.set_yticks([])
            last_cf=cf
    if last_cf is not None:
        fig.subplots_adjust(right=0.88, top=0.94)
        cax = fig.add_axes([0.90, 0.12, 0.02, 0.72])
        fig.colorbar(last_cf, cax=cax, label='Sea Level Pressure Anomaly (hPa)')
    plt.suptitle(title, fontsize=14)
    plt.tight_layout(rect=[0,0,0.88,0.94])
    plt.savefig(save_path, dpi=300)
    plt.close(fig)


def save_each_node_mean_image(data_flat, winners_xy, lat, lon, som_shape, out_dir, prefix):
    """
    ノード平均（セントロイド）の個別図を保存
    """
    os.makedirs(out_dir, exist_ok=True)
    H, W = len(lat), len(lon)
    X2 = data_flat.reshape(-1, H, W)  # 偏差[hPa]
    map_x, map_y = som_shape

    vmin, vmax = -40, 40
    levels = np.linspace(vmin, vmax, 21)
    cmap = 'RdBu_r'

    for ix in range(map_x):
        for iy in range(map_y):
            mask = (winners_xy[:,0]==ix) & (winners_xy[:,1]==iy)
            idxs = np.where(mask)[0]
            if len(idxs)>0:
                mean_img = np.nanmean(X2[idxs], axis=0)
            else:
                mean_img = np.full((H,W), np.nan, dtype=np.float32)

            fig = plt.figure(figsize=(4,3))
            ax = plt.axes(projection=ccrs.PlateCarree())
            cf = ax.contourf(lon, lat, mean_img, levels=levels, cmap=cmap, transform=ccrs.PlateCarree(), extend='both')
            ax.contour(lon, lat, mean_img, levels=levels, colors='k', linewidths=0.3, transform=ccrs.PlateCarree())
            ax.add_feature(cfeature.COASTLINE.with_scale('50m'), linewidth=0.8, edgecolor='black')
            ax.set_extent([115, 155, 15, 55], ccrs.PlateCarree())
            plt.colorbar(cf, ax=ax, fraction=0.046, pad=0.04, label='Sea Level Pressure Anomaly (hPa)')
            ax.set_title(f'({ix},{iy}) N={len(idxs)}')
            ax.set_xticks([]); ax.set_yticks([])
            fpath = os.path.join(out_dir, f'{prefix}_node_{ix}_{iy}.png')
            plt.tight_layout()
            plt.savefig(fpath, dpi=180)
            plt.close(fig)


def plot_label_distributions_base(winners_xy, labels_raw: List[Optional[str]],
                                  base_labels: List[str], som_shape: Tuple[int,int],
                                  save_dir: str, title_prefix: str):
    """
    基本ラベルのみの分布ヒートマップ（15種類を1枚にまとめる）
    """
    os.makedirs(save_dir, exist_ok=True)
    node_counts = {lbl: np.zeros((som_shape[0], som_shape[1]), dtype=int) for lbl in base_labels}
    for i,(ix,iy) in enumerate(winners_xy):
        lab = basic_label_or_none(labels_raw[i], base_labels)
        if lab in node_counts:
            node_counts[lab][ix,iy] += 1
    cols = 5
    rows = int(np.ceil(len(base_labels)/cols))
    fig, axes = plt.subplots(rows, cols, figsize=(cols*2.6, rows*2.6))
    axes = np.atleast_2d(axes)
    for idx,lbl in enumerate(base_labels):
        r = idx//cols; c=idx%cols
        ax = axes[r,c]
        im = ax.imshow(node_counts[lbl].T[::-1,:], cmap='viridis')
        ax.set_title(lbl); ax.set_xticks([]); ax.set_yticks([])
        fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)
    for k in range(len(base_labels), rows*cols):
        r = k//cols; c=k%cols
        axes[r,c].axis('off')
    plt.suptitle(f'{title_prefix} Label Distributions on SOM nodes (Base only)', fontsize=14)
    plt.tight_layout(rect=[0,0,1,0.95])
    fpath = os.path.join(save_dir, f'{title_prefix}_label_distributions_base.png')
    plt.savefig(fpath, dpi=250)
    plt.close(fig)


def save_label_distributions_base_individual(winners_xy, labels_raw: List[Optional[str]],
                                             base_labels: List[str], som_shape: Tuple[int,int],
                                             save_dir: str, title_prefix: str):
    """
    基本ラベルのみの分布ヒートマップ（各ラベルごとの個別画像を追加保存）
    """
    os.makedirs(save_dir, exist_ok=True)
    node_counts = {lbl: np.zeros((som_shape[0], som_shape[1]), dtype=int) for lbl in base_labels}
    for i,(ix,iy) in enumerate(winners_xy):
        lab = basic_label_or_none(labels_raw[i], base_labels)
        if lab in node_counts:
            node_counts[lab][ix,iy] += 1
    for lbl in base_labels:
        fig = plt.figure(figsize=(4, 3))
        ax = plt.gca()
        im = ax.imshow(node_counts[lbl].T[::-1,:], cmap='viridis')
        ax.set_title(lbl)
        ax.set_xticks([]); ax.set_yticks([])
        plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)
        fpath = os.path.join(save_dir, f'{title_prefix}_label_dist_base_{lbl}.png')
        plt.tight_layout()
        plt.savefig(fpath, dpi=200)
        plt.close(fig)


def analyze_nodes_detail_to_log(clusters: List[List[int]],
                                labels: List[Optional[str]],
                                timestamps: np.ndarray,
                                base_labels: List[str],
                                som_shape: Tuple[int, int],
                                log: Logger,
                                title: str):
    """
    ノードごとの詳細（基本ラベル構成・月別分布・純度・代表ラベル[raw]）を results.log に追記。
    """
    log.write(f'\n--- {title} ---\n')
    for k, idxs in enumerate(clusters):
        ix, iy = k // som_shape[1], k % som_shape[1]
        n = len(idxs)
        if n == 0:
            continue
        log.write(f'\n[Node ({ix},{iy})] N={n}\n')
        # 代表ラベル（元ラベル：複合含む）
        cnt_raw = Counter([labels[j] for j in idxs if labels[j] is not None])
        if len(cnt_raw) > 0:
            top_raw, top_count_raw = cnt_raw.most_common(1)[0]
            log.write(f'  - 代表ラベル（元ラベル）: {top_raw}  ({top_count_raw}/{n}, {top_count_raw/n*100:5.1f}%)\n')

        # 基本ラベル構成
        cnt = Counter()
        for j in idxs:
            bl = basic_label_or_none(labels[j], base_labels)
            if bl is not None:
                cnt[bl] += 1
        if cnt:
            log.write('  - ラベル構成（基本ラベルのみ）:\n')
            for lbl, c in sorted(cnt.items(), key=lambda x: x[1], reverse=True):
                log.write(f'    {lbl:<3}: {c:4d} ({c/n*100:5.1f}%)\n')
            purity = max(cnt.values()) / n
            log.write(f'  - ノード純度（基本ラベル多数決）: {purity:.3f}\n')

        # 月別分布
        if timestamps is not None and n > 0:
            months = pd.to_datetime(timestamps[idxs]).month
            mon_c = Counter(months)
            log.write('  - 月別分布:\n')
            for m in range(1, 13):
                c = mon_c.get(m, 0)
                log.write(f'    {m:2d}月: {c:4d} ({c/n*100:5.1f}%)\n')
    log.write(f'--- {title} 終了 ---\n')


def log_som_recall_by_label_with_nodes(
    log: Logger,
    winners_xy: np.ndarray,
    labels_all: List[Optional[str]],
    base_labels: List[str],
    som_shape: Tuple[int, int],
    section_title: str
):
    if labels_all is None or len(labels_all) == 0:
        log.write("\nラベルが無いため、代表ノード群ベースの再現率出力をスキップします。\n")
        return

    H_nodes, W_nodes = som_shape
    n_nodes = H_nodes * W_nodes
    node_index_arr = winners_xy[:, 0] * W_nodes + winners_xy[:, 1]  # (N,)

    # ノード毎の基本ラベル分布
    node_counters = [Counter() for _ in range(n_nodes)]
    for i, k in enumerate(node_index_arr):
        bl = basic_label_or_none(labels_all[i], base_labels)
        if bl is not None:
            node_counters[int(k)][bl] += 1

    # ノードの代表（多数決）ラベル
    node_majority: List[Optional[str]] = [None] * n_nodes
    for k in range(n_nodes):
        if len(node_counters[k]) > 0:
            node_majority[k] = node_counters[k].most_common(1)[0][0]

    # ラベル→代表ノード一覧
    label_to_nodes: Dict[str, List[Tuple[int, int]]] = {lbl: [] for lbl in base_labels}
    for k, rep in enumerate(node_majority):
        if rep is None:
            continue
        ix, iy = k // W_nodes, k % W_nodes
        label_to_nodes[rep].append((ix, iy))

    # 基本ラベルベースの再現率
    total_base = Counter()
    correct_base = Counter()
    for i, k in enumerate(node_index_arr):
        bl = basic_label_or_none(labels_all[i], base_labels)
        if bl is None:
            continue
        total_base[bl] += 1
        pred = node_majority[int(k)]
        if pred is None:
            continue
        if pred == bl:
            correct_base[bl] += 1

    # 複合ラベル考慮（基本+応用）
    total_comp = Counter()
    correct_comp = Counter()
    for i, k in enumerate(node_index_arr):
        comps = extract_base_components(labels_all[i], base_labels)
        if not comps:
            continue
        for c in comps:
            total_comp[c] += 1
        pred = node_majority[int(k)]
        if pred is None:
            continue
        if pred in comps:
            correct_comp[pred] += 1

    log.write(f"\n【{section_title}】\n")
    log.write("【各ラベルの再現率（代表ノード群ベース）】\n")
    recalls_base = []
    for lbl in base_labels:
        N = int(total_base[lbl])
        C = int(correct_base[lbl])
        rec = (C / N) if N > 0 else 0.0
        recalls_base.append(rec if N > 0 else np.nan)
        nodes_disp = label_to_nodes.get(lbl, [])
        rep_str = "なし" if len(nodes_disp) == 0 else "[" + ", ".join([f"({ix},{iy})" for ix, iy in nodes_disp]) + "]"
        log.write(f" - {lbl:<3}: N={N:4d} Correct={C:4d} Recall={rec:.4f} 代表={rep_str}\n")

    log.write("\n【複合ラベル考慮の再現率（基本+応用）】\n")
    recalls_comp = []
    for lbl in base_labels:
        Nt = int(total_comp[lbl])
        Ct = int(correct_comp[lbl])
        rec_t = (Ct / Nt) if Nt > 0 else 0.0
        if Nt > 0:
            recalls_comp.append(rec_t)
        log.write(f" - {lbl:<3}: N={Nt:4d} Correct={Ct:4d} Recall={rec_t:.4f}\n")

    base_valid = [r for r, lbl in zip(recalls_base, base_labels) if not np.isnan(r) and total_base[lbl] > 0]
    macro_base = float(np.mean(base_valid)) if len(base_valid) > 0 else float('nan')
    macro_comp = float(np.mean(recalls_comp)) if len(recalls_comp) > 0 else float('nan')
    log.write(f"\n[Summary] Macro Recall (基本ラベル)   = {macro_base:.4f}\n")
    log.write(f"[Summary] Macro Recall (基本+応用) = {macro_comp:.4f}\n")


# =====================================================
# medoid（ノード重心に最も近いサンプル）を計算・保存する処理
# =====================================================
def _euclidean_dist_to_ref(Xb: torch.Tensor, ref: torch.Tensor) -> torch.Tensor:
    # Xb: (B,H,W), ref: (H,W) -> (B,)
    diff = Xb - ref.view(1, *ref.shape)
    d2 = (diff*diff).sum(dim=(1,2))
    return torch.sqrt(d2 + 1e-12)


def _ssim_dist_to_ref(Xb: torch.Tensor, ref: torch.Tensor, c1: float = SSIM_C1, c2: float = SSIM_C2) -> torch.Tensor:
    # 1 - SSIM (全体1窓の簡略SSIM)
    B,H,W = Xb.shape
    mu_x = Xb.mean(dim=(1,2))              # (B,)
    xc = Xb - mu_x.view(B,1,1)
    var_x = (xc*xc).mean(dim=(1,2))        # (B,)
    mu_r = ref.mean()
    rc = ref - mu_r
    var_r = (rc*rc).mean()                 # scalar
    cov = (xc * rc.view(1,H,W)).mean(dim=(1,2))  # (B,)
    l_num = (2*mu_x*mu_r + c1)
    l_den = (mu_x**2 + mu_r**2 + c1)
    c_num = (2*cov + c2)
    c_den = (var_x + var_r + c2)
    ssim = (l_num*c_num) / (l_den*c_den + 1e-12)
    return 1.0 - ssim


def _ssim5_dist_to_ref(Xb: torch.Tensor, ref: torch.Tensor) -> torch.Tensor:
    """
    1 - mean(SSIM_map) with 5x5 moving window, C1=C2=0 (denominator epsilon guarded)
    """
    B, H, W = Xb.shape
    device = Xb.device
    dtype = Xb.dtype
    pad = 2
    kernel = torch.ones((1, 1, 5, 5), device=device, dtype=dtype) / 25.0

    X = Xb.unsqueeze(1)              # (B,1,H,W)
    R = ref.view(1, 1, H, W)         # (1,1,H,W)

    X_pad = F.pad(X, (pad, pad, pad, pad), mode='reflect')
    R_pad = F.pad(R, (pad, pad, pad, pad), mode='reflect')

    mu_x = F.conv2d(X_pad, kernel, padding=0)            # (B,1,H,W)
    mu_r = F.conv2d(R_pad, kernel, padding=0)            # (1,1,H,W)

    mu_x2 = F.conv2d(X_pad * X_pad, kernel, padding=0)
    mu_r2 = F.conv2d(R_pad * R_pad, kernel, padding=0)
    var_x = torch.clamp(mu_x2 - mu_x * mu_x, min=0.0)
    var_r = torch.clamp(mu_r2 - mu_r * mu_r, min=0.0)

    prod = X * R
    prod_pad = F.pad(prod, (pad, pad, pad, pad), mode='reflect')
    mu_xr = F.conv2d(prod_pad, kernel, padding=0)
    cov = mu_xr - mu_x * mu_r

    eps = 1e-12
    l_num = 2 * (mu_x * mu_r)
    l_den = (mu_x * mu_x + mu_r * mu_r)
    c_num = 2 * cov
    c_den = (var_x + var_r)
    ssim_map = (l_num * c_num) / (l_den * c_den + eps)    # (B,1,H,W)
    ssim_avg = ssim_map.mean(dim=(1, 2, 3))               # (B,)
    return 1.0 - ssim_avg

def _s1_dist_to_ref(Xb: torch.Tensor, ref: torch.Tensor) -> torch.Tensor:
    # Teweles–Wobus S1: 100 * sum|∇X-∇ref| / sum max(|∇X|,|∇ref|)
    dXdx = Xb[:,:,1:] - Xb[:,:,:-1]
    dXdy = Xb[:,1:,:] - Xb[:,:-1,:]
    dRdx = ref[:,1:] - ref[:,:-1]
    dRdy = ref[1:,:] - ref[:-1,:]
    num_dx = (torch.abs(dXdx - dRdx.view(1, *dRdx.shape))).sum(dim=(1,2))
    num_dy = (torch.abs(dXdy - dRdy.view(1, *dRdy.shape))).sum(dim=(1,2))
    den_dx = torch.maximum(torch.abs(dXdx), torch.abs(dRdx).view(1, *dRdx.shape)).sum(dim=(1,2))
    den_dy = torch.maximum(torch.abs(dXdy), torch.abs(dRdy).view(1, *dRdy.shape)).sum(dim=(1,2))
    s1 = 100.0 * (num_dx + num_dy) / (den_dx + den_dy + 1e-12)
    return s1


def compute_node_medoids_by_centroid(
    method_name: str,
    data_flat: np.ndarray,          # (N,D) hPa偏差（空間平均差し引き）
    winners_xy: np.ndarray,         # (N,2)
    som_shape: Tuple[int,int],
    field_shape: Tuple[int,int],
    device: str = 'cuda' if torch.cuda.is_available() else 'cpu'
) -> Tuple[Dict[Tuple[int,int], int], Dict[Tuple[int,int], float]]:
    """
    各ノードの「重心（平均画像）」に最も近いサンプルを、方式別距離で選ぶ。
    戻り値:
      - node_to_medoid_idx: {(ix,iy): sample_index}
      - node_to_medoid_dist: {(ix,iy): distance_value}
    """
    H,W = field_shape
    X2 = data_flat.reshape(-1, H, W)
    X_t = torch.as_tensor(X2, device=device, dtype=torch.float32)

    node_to_medoid_idx: Dict[Tuple[int,int], int] = {}
    node_to_medoid_dist: Dict[Tuple[int,int], float] = {}

    for ix in range(som_shape[0]):
        for iy in range(som_shape[1]):
            mask = (winners_xy[:,0]==ix) & (winners_xy[:,1]==iy)
            idxs = np.where(mask)[0]
            if len(idxs) == 0:
                continue
            # 重心（平均画像）
            centroid = np.nanmean(X2[idxs], axis=0).astype(np.float32)  # (H,W)
            centroid_t = torch.as_tensor(centroid, device=device, dtype=torch.float32)
            # 距離
            Xb = X_t[idxs]  # (B,H,W)
            if method_name == 'euclidean':
                d = _euclidean_dist_to_ref(Xb, centroid_t)
            elif method_name == 'ssim':
                d = _ssim_dist_to_ref(Xb, centroid_t)
            elif method_name == 'ssim5':
                d = _ssim5_dist_to_ref(Xb, centroid_t)
            elif method_name == 's1':
                d = _s1_dist_to_ref(Xb, centroid_t)
            elif method_name == 's1ssim':
                d1 = _s1_dist_to_ref(Xb, centroid_t)
                d2 = _ssim5_dist_to_ref(Xb, centroid_t)
                min1, _ = torch.min(d1, dim=0, keepdim=True)
                max1, _ = torch.max(d1, dim=0, keepdim=True)
                min2, _ = torch.min(d2, dim=0, keepdim=True)
                max2, _ = torch.max(d2, dim=0, keepdim=True)
                dn1 = (d1 - min1) / (max1 - min1 + 1e-12)
                dn2 = (d2 - min2) / (max2 - min2 + 1e-12)
                d = 0.5 * (dn1 + dn2)
            else:
                raise ValueError(f'Unknown method_name: {method_name}')
            # 最小
            pos = int(torch.argmin(d).item())
            node_to_medoid_idx[(ix,iy)] = int(idxs[pos])
            node_to_medoid_dist[(ix,iy)] = float(d[pos].item())

    return node_to_medoid_idx, node_to_medoid_dist


def compute_node_true_medoids(
    method_name: str,
    data_flat: np.ndarray,
    winners_xy: np.ndarray,
    som_shape: Tuple[int, int],
    field_shape: Tuple[int, int],
    device: str = 'cuda' if torch.cuda.is_available() else 'cpu',
    fusion_alpha: float = 0.5
) -> Tuple[Dict[Tuple[int,int], int], Dict[Tuple[int,int], float]]:
    """
    各ノードについて「総距離最小（true medoid）」のサンプルを選ぶ。
    - 各ノードの割当集合 Ic の中で、候補 i∈Ic について cost(i) = Σ_{j∈Ic} d(X_j, X_i) を計算し最小の i を選ぶ。
    - method_name: 'euclidean' | 'ssim' | 'ssim5' | 's1' | 's1ssim'
    - 戻りの距離は、選ばれたメドイドに対する平均距離（sum/|Ic|）。
    """
    H, W = field_shape
    X2 = data_flat.reshape(-1, H, W)
    X_t = torch.as_tensor(X2, device=device, dtype=torch.float32)

    node_to_medoid_idx: Dict[Tuple[int,int], int] = {}
    node_to_medoid_avgdist: Dict[Tuple[int,int], float] = {}

    def pairwise_euclidean(Xc: torch.Tensor) -> torch.Tensor:
        C = Xc.shape[0]
        Xf = Xc.reshape(C, -1)
        x2 = (Xf * Xf).sum(dim=1, keepdim=True)           # (C,1)
        d2 = x2 + x2.T - 2.0 * (Xf @ Xf.T)                # (C,C)
        d2 = torch.clamp(d2, min=0.0)
        return torch.sqrt(d2 + 1e-12)

    def pairwise_by_ref(dist_fn, Xc: torch.Tensor) -> torch.Tensor:
        # 行ごとに「ref=Xc[i]」として距離ベクトルを計算して積み上げる
        C = Xc.shape[0]
        D = torch.empty((C, C), device=Xc.device, dtype=Xc.dtype)
        for i in range(C):
            ref = Xc[i]
            D[i] = dist_fn(Xc, ref)  # (C,)
        return D

    for ix in range(som_shape[0]):
        for iy in range(som_shape[1]):
            mask = (winners_xy[:, 0] == ix) & (winners_xy[:, 1] == iy)
            idxs = np.where(mask)[0]
            if len(idxs) == 0:
                continue
            Xcand = X_t[idxs]  # (C,H,W)
            Ccand = Xcand.shape[0]

            if Ccand == 1:
                node_to_medoid_idx[(ix, iy)] = int(idxs[0])
                node_to_medoid_avgdist[(ix, iy)] = 0.0
                continue

            # 距離行列 D (C,C) を作る
            if method_name == 'euclidean':
                D = pairwise_euclidean(Xcand)
            elif method_name == 'ssim':
                D = pairwise_by_ref(lambda Xb, ref: _ssim_dist_to_ref(Xb, ref), Xcand)
            elif method_name == 'ssim5':
                D = pairwise_by_ref(lambda Xb, ref: _ssim5_dist_to_ref(Xb, ref), Xcand)
            elif method_name == 's1':
                D = pairwise_by_ref(lambda Xb, ref: _s1_dist_to_ref(Xb, ref), Xcand)
            elif method_name == 's1ssim':
                # まず個別の距離行列
                D1 = pairwise_by_ref(lambda Xb, ref: _s1_dist_to_ref(Xb, ref), Xcand)
                D2 = pairwise_by_ref(lambda Xb, ref: _ssim5_dist_to_ref(Xb, ref), Xcand)
                # 行ごとにmin-max正規化（i行に対して）
                eps = 1e-12
                d1_min = D1.min(dim=1, keepdim=True).values
                d1_max = D1.max(dim=1, keepdim=True).values
                d2_min = D2.min(dim=1, keepdim=True).values
                d2_max = D2.max(dim=1, keepdim=True).values
                D1n = (D1 - d1_min) / (d1_max - d1_min + eps)
                D2n = (D2 - d2_min) / (d2_max - d2_min + eps)
                D = fusion_alpha * D1n + (1.0 - fusion_alpha) * D2n
            else:
                raise ValueError(f'Unknown method_name: {method_name}')

            # cost(i) = Σ_j D[i,j]
            costs = D.sum(dim=1)  # (C,)
            imin = int(torch.argmin(costs).item())
            node_to_medoid_idx[(ix, iy)] = int(idxs[imin])
            node_to_medoid_avgdist[(ix, iy)] = float((costs[imin] / Ccand).item())

    return node_to_medoid_idx, node_to_medoid_avgdist


def plot_som_node_medoid_patterns(
    data_flat: np.ndarray,
    node_to_medoid_idx: Dict[Tuple[int,int], int],
    lat: np.ndarray, lon: np.ndarray,
    som_shape: Tuple[int,int],
    times_all: Optional[np.ndarray],
    save_path: str,
    title: str
):
    """
    ノードmedoid（重心に最も近いサンプル）のマップを保存
    各サブプロットのタイトルを (x,y)_YYYY/MM/DD にする。
    """
    H, W = len(lat), len(lon)

    vmin, vmax = -40, 40
    levels = np.linspace(vmin, vmax, 21)
    cmap = 'RdBu_r'

    map_x, map_y = som_shape
    nrows, ncols = map_y, map_x
    figsize=(ncols*2.6, nrows*2.6)
    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=figsize,
                             subplot_kw={'projection': ccrs.PlateCarree()})
    axes = np.atleast_2d(axes)
    axes = axes.T[::-1,:]  # 表示並びの整合

    last_cf = None
    for ix in range(map_x):
        for iy in range(map_y):
            ax = axes[ix, iy]
            key = (ix, iy)
            if key not in node_to_medoid_idx:
                ax.set_axis_off()
                continue
            mi = node_to_medoid_idx[key]
            pat = data_flat[mi].reshape(H, W)
            cf = ax.contourf(lon, lat, pat, levels=levels, cmap=cmap, extend='both', transform=ccrs.PlateCarree())
            ax.contour(lon, lat, pat, levels=levels, colors='k', linewidths=0.3, transform=ccrs.PlateCarree())
            ax.add_feature(cfeature.COASTLINE.with_scale('50m'), edgecolor='black', linewidth=0.8)
            ax.set_extent([115, 155, 15, 55], ccrs.PlateCarree())
            # タイトルに日付を追加（YYYY/MM/DD）
            dstr = format_date_yyyymmdd(times_all[mi]) if times_all is not None and len(times_all)>mi else ''
            ax.set_title(f'({ix},{iy})_{dstr}')
            ax.set_xticks([]); ax.set_yticks([])
            last_cf = cf

    if last_cf is not None:
        fig.subplots_adjust(right=0.88, top=0.94)
        cax = fig.add_axes([0.90, 0.12, 0.02, 0.72])
        fig.colorbar(last_cf, cax=cax, label='Sea Level Pressure Anomaly (hPa)')
    plt.suptitle(title, fontsize=14)
    plt.tight_layout(rect=[0,0,0.88,0.94])
    plt.savefig(save_path, dpi=300)
    plt.close(fig)


def save_each_node_medoid_image(
    data_flat: np.ndarray,
    node_to_medoid_idx: Dict[Tuple[int,int], int],
    lat: np.ndarray, lon: np.ndarray,
    som_shape: Tuple[int,int],
    out_dir: str,
    prefix: str
):
    os.makedirs(out_dir, exist_ok=True)
    H, W = len(lat), len(lon)

    vmin, vmax = -40, 40
    levels = np.linspace(vmin, vmax, 21)
    cmap = 'RdBu_r'

    for ix in range(som_shape[0]):
        for iy in range(som_shape[1]):
            key = (ix, iy)
            if key not in node_to_medoid_idx:
                continue
            mi = node_to_medoid_idx[key]
            pat = data_flat[mi].reshape(H, W)
            fig = plt.figure(figsize=(4,3))
            ax = plt.axes(projection=ccrs.PlateCarree())
            cf = ax.contourf(lon, lat, pat, levels=levels, cmap=cmap, transform=ccrs.PlateCarree(), extend='both')
            ax.contour(lon, lat, pat, levels=levels, colors='k', linewidths=0.3, transform=ccrs.PlateCarree())
            ax.add_feature(cfeature.COASTLINE.with_scale('50m'), linewidth=0.8, edgecolor='black')
            ax.set_extent([115, 155, 15, 55], ccrs.PlateCarree())
            plt.colorbar(cf, ax=ax, fraction=0.046, pad=0.04, label='Sea Level Pressure Anomaly (hPa)')
            ax.set_title(f'Medoid ({ix},{iy}) sample={mi}')
            ax.set_xticks([]); ax.set_yticks([])
            fpath = os.path.join(out_dir, f'{prefix}_node_{ix}_{iy}_medoid.png')
            plt.tight_layout()
            plt.savefig(fpath, dpi=180)
            plt.close(fig)


def plot_nodewise_match_map(
    winners_xy: np.ndarray,
    labels_all: List[str],
    node_to_medoid_idx: Dict[Tuple[int,int], int],
    times_all: np.ndarray,
    som_shape: Tuple[int,int],
    save_path: str,
    title: str
):
    """
    各ノードについて、
      - ノード内の最頻出ラベル（元ラベル、複合含む）= majority
      - medoid の元ラベル
    が一致すれば「薄い緑」でノード四角を塗り、不一致なら「薄い赤」にする。
    N, majority, medoid, date(YYYY/MM/DD) を少し大きめに表示。
    """
    # ノードごとのサンプルリスト
    clusters = winners_to_clusters(winners_xy, som_shape)
    node_to_majority_raw: Dict[Tuple[int,int], Optional[str]] = {}
    node_to_count: Dict[Tuple[int,int], int] = {}

    for k, idxs in enumerate(clusters):
        ix, iy = k // som_shape[1], k % som_shape[1]
        node_to_count[(ix,iy)] = len(idxs)
        if len(idxs) == 0:
            node_to_majority_raw[(ix,iy)] = None
            continue
        cnt = Counter([labels_all[j] for j in idxs if labels_all[j] is not None])
        if len(cnt) == 0:
            node_to_majority_raw[(ix,iy)] = None
        else:
            node_to_majority_raw[(ix,iy)] = cnt.most_common(1)[0][0]

    # グリッド図
    map_x, map_y = som_shape
    nrows, ncols = map_y, map_x
    figsize = (ncols*2.6, nrows*2.6)
    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=figsize)
    axes = np.atleast_2d(axes)
    axes = axes.T[::-1, :]

    for ix in range(map_x):
        for iy in range(map_y):
            ax = axes[ix, iy]
            ax.set_xticks([]); ax.set_yticks([])
            maj = node_to_majority_raw.get((ix,iy), None)
            if (ix,iy) not in node_to_medoid_idx:
                # データ無し
                ax.set_facecolor((0.95, 0.95, 0.95, 1.0))  # 薄い灰色
                ax.text(0.5, 0.5, '-', ha='center', va='center', fontsize=16, color='gray')
                ax.text(0.02, 0.98, f'({ix},{iy}) N={node_to_count.get((ix,iy),0)}',
                        transform=ax.transAxes, ha='left', va='top', fontsize=10)
                continue
            mi = node_to_medoid_idx[(ix,iy)]
            med_raw = labels_all[mi] if labels_all is not None else None
            dstr = format_date_yyyymmdd(times_all[mi]) if times_all is not None and len(times_all)>mi else ''
            match = (med_raw == maj) if (med_raw is not None and maj is not None) else False

            # 背景色を一致/不一致で塗る（薄い緑/薄い赤）
            if match:
                ax.set_facecolor((0.85, 1.00, 0.85, 1.0))  # light green
            else:
                ax.set_facecolor((1.00, 0.85, 0.85, 1.0))  # light red

            # ヘッダ
            ax.text(0.02, 0.98, f'({ix},{iy}) N={node_to_count.get((ix,iy),0)}',
                    transform=ax.transAxes, ha='left', va='top', fontsize=10, fontweight='bold')
            # 付記（少し大きめ）
            ax.text(0.02, 0.78, f'Majority: {maj}', transform=ax.transAxes, ha='left', va='top', fontsize=10)
            ax.text(0.02, 0.62, f'Medoid  : {med_raw}', transform=ax.transAxes, ha='left', va='top', fontsize=10)
            ax.text(0.02, 0.46, f'Date    : {dstr}', transform=ax.transAxes, ha='left', va='top', fontsize=10)

    plt.suptitle(title, fontsize=14)
    plt.tight_layout(rect=[0,0,1,0.95])
    plt.savefig(save_path, dpi=300)
    plt.close(fig)


# =====================================================
# Nodewise match rate（rawラベルでの一致率）計算ユーティリティ
# =====================================================
def compute_nodewise_match_rate(
    winners_xy: np.ndarray,
    labels_all: List[Optional[str]],
    node_to_medoid_idx: Dict[Tuple[int,int], int],
    som_shape: Tuple[int, int]
) -> Tuple[float, int, int]:
    """
    ノード代表ラベル（raw: 複合含む）と medoid の raw ラベルの一致率。
    戻り値: (match_rate, matched_nodes, counted_nodes)
    """
    clusters = winners_to_clusters(winners_xy, som_shape)
    node_to_majority_raw: Dict[Tuple[int,int], Optional[str]] = {}
    for k, idxs in enumerate(clusters):
        ix, iy = k // som_shape[1], k % som_shape[1]
        if len(idxs) == 0:
            node_to_majority_raw[(ix,iy)] = None
        else:
            cnt_raw = Counter([labels_all[j] for j in idxs if labels_all[j] is not None])
            node_to_majority_raw[(ix,iy)] = cnt_raw.most_common(1)[0][0] if len(cnt_raw)>0 else None

    matched = 0
    counted = 0
    for key, mi in node_to_medoid_idx.items():
        maj = node_to_majority_raw.get(key, None)
        med_raw = labels_all[mi] if labels_all is not None else None
        if maj is None or med_raw is None:
            continue
        counted += 1
        if maj == med_raw:
            matched += 1
    rate = (matched / counted) if counted > 0 else np.nan
    return float(rate), matched, counted


# =====================================================
# 学習時のノード代表（raw/基本）ラベルを構築
# =====================================================
def compute_training_node_majorities(
    winners_xy: np.ndarray,
    labels_all: List[Optional[str]],
    base_labels: List[str],
    som_shape: Tuple[int, int]
) -> Tuple[Dict[Tuple[int,int], Optional[str]], Dict[Tuple[int,int], Optional[str]]]:
    """
    学習データ上で、各ノードの代表ラベルを raw と基本の2種類で計算
    戻り値: (node_to_majority_raw, node_to_majority_base)
    """
    clusters = winners_to_clusters(winners_xy, som_shape)
    node_to_majority_raw: Dict[Tuple[int,int], Optional[str]] = {}
    node_to_majority_base: Dict[Tuple[int,int], Optional[str]] = {}

    for k, idxs in enumerate(clusters):
        ix, iy = k // som_shape[1], k % som_shape[1]
        # raw
        cnt_raw = Counter([labels_all[j] for j in idxs if labels_all[j] is not None])
        node_to_majority_raw[(ix, iy)] = cnt_raw.most_common(1)[0][0] if len(cnt_raw) > 0 else None
        # base
        cnt_base = Counter()
        for j in idxs:
            bl = basic_label_or_none(labels_all[j], base_labels)
            if bl is not None:
                cnt_base[bl] += 1
        node_to_majority_base[(ix, iy)] = cnt_base.most_common(1)[0][0] if len(cnt_base) > 0 else None

    return node_to_majority_raw, node_to_majority_base


# =====================================================
# 検証（学習時代表ラベルに基づく推論）ユーティリティ
# =====================================================
def evaluate_verification_with_training_majority(
    winners_xy_valid: np.ndarray,
    labels_valid: List[Optional[str]],
    times_valid: np.ndarray,
    base_labels: List[str],
    som_shape: Tuple[int, int],
    node_to_majority_base_train: Dict[Tuple[int,int], Optional[str]],
    out_dir: str,
    method_name: str,
    logger: Logger
):
    """
    学習時の「ノード代表（基本ラベル）」を予測ラベルとして使用し、検証データの正解率を評価。
    - 混同行列（基本ラベル vs クラスタ列）CSV
    - per-label 再現率（基本/複合）CSV
    - 割当CSV（予測/正誤フラグ含む）
    - 棒グラフ（基本/複合）PNG
    """
    os.makedirs(out_dir, exist_ok=True)
    Hn, Wn = som_shape

    # 混同行列（基本ラベル vs クラスタ列）: 検証データベース
    clusters_val = winners_to_clusters(winners_xy_valid, som_shape)
    cm_val, cluster_names = build_confusion_matrix_only_base(clusters_val, labels_valid, base_labels)
    conf_csv = os.path.join(out_dir, f'{method_name}_verification_confusion_matrix.csv')
    cm_val.to_csv(conf_csv, encoding='utf-8-sig')

    # 学習代表（基本）を予測ラベルとして、検証の per-label 再現率（基本/複合）を算出
    total_base = Counter()
    correct_base = Counter()
    total_comp = Counter()
    correct_comp = Counter()

    # 割当CSV用
    rows_assign = []

    for i, (ix, iy) in enumerate(winners_xy_valid):
        # 実ラベル（基本/複合）
        raw = labels_valid[i]
        bl = basic_label_or_none(raw, base_labels)
        comps = extract_base_components(raw, base_labels)

        # 予測（学習時の代表基本ラベル）
        pred = node_to_majority_base_train.get((int(ix), int(iy)), None)

        # 基本ラベル再現率用
        if bl is not None:
            total_base[bl] += 1
            if pred is not None and pred == bl:
                correct_base[bl] += 1

        # 複合ラベル再現率用
        if comps:
            for c in comps:
                total_comp[c] += 1
            if pred is not None and pred in comps:
                correct_comp[pred] += 1

        # 割当CSV 1行
        rows_assign.append({
            'time': format_date_yyyymmdd(times_valid[i]),
            'bmu_x': int(ix), 'bmu_y': int(iy),
            'label_raw': raw if raw is not None else '',
            'actual_base': bl if bl is not None else '',
            'pred_base_from_train': pred if pred is not None else '',
            'correct_base': int(1 if (bl is not None and pred == bl) else 0),
            'correct_composite': int(1 if (pred is not None and pred in comps) else 0)
        })

    # per-label 再現率テーブルとマクロ平均
    per_label_rows = []
    recalls_base = []
    recalls_comp = []
    for lbl in base_labels:
        N_base = int(total_base[lbl])
        C_base = int(correct_base[lbl])
        rec_base = (C_base / N_base) if N_base > 0 else np.nan

        N_comp = int(total_comp[lbl])
        C_comp = int(correct_comp[lbl])
        rec_comp = (C_comp / N_comp) if N_comp > 0 else np.nan

        per_label_rows.append({
            'label': lbl,
            'N_base': N_base, 'Correct_base': C_base, 'Recall_base': rec_base,
            'N_composite': N_comp, 'Correct_composite': C_comp, 'Recall_composite': rec_comp
        })
        if not np.isnan(rec_base):
            recalls_base.append(rec_base)
        if not np.isnan(rec_comp):
            recalls_comp.append(rec_comp)

    macro_base = float(np.mean(recalls_base)) if len(recalls_base) > 0 else float('nan')
    macro_comp = float(np.mean(recalls_comp)) if len(recalls_comp) > 0 else float('nan')

    # 保存（CSV）
    assign_csv = os.path.join(out_dir, f'{method_name}_verification_assign.csv')
    pd.DataFrame(rows_assign).to_csv(assign_csv, index=False, encoding='utf-8-sig')

    per_label_csv = os.path.join(out_dir, f'{method_name}_verification_per_label_recall.csv')
    df_pl = pd.DataFrame(per_label_rows)
    df_pl.to_csv(per_label_csv, index=False, encoding='utf-8-sig')

    # 棒グラフ出力
    def plot_per_label_bars(labels, values, title, out_png):
        fig = plt.figure(figsize=(max(8, 0.5*len(labels)+2), 4))
        ax = plt.gca()
        ax.bar(labels, values, color='steelblue')
        ax.set_ylim(0, 1.0)
        ax.set_ylabel('Recall')
        ax.set_title(title)
        for i, v in enumerate(values):
            if not np.isnan(v):
                ax.text(i, v+0.02, f"{v:.2f}", ha='center', va='bottom', fontsize=8)
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.savefig(out_png, dpi=200)
        plt.close(fig)

    base_vals = [float(df_pl.loc[df_pl['label']==lbl, 'Recall_base'].values[0]) if lbl in df_pl['label'].values else np.nan for lbl in base_labels]
    comp_vals = [float(df_pl.loc[df_pl['label']==lbl, 'Recall_composite'].values[0]) if lbl in df_pl['label'].values else np.nan for lbl in base_labels]
    plot_per_label_bars(base_labels, base_vals, f'{method_name.upper()} Verification Recall (Base)', os.path.join(out_dir, f'{method_name}_verification_per_label_recall_base.png'))
    plot_per_label_bars(base_labels, comp_vals, f'{method_name.upper()} Verification Recall (Composite)', os.path.join(out_dir, f'{method_name}_verification_per_label_recall_composite.png'))

    # ログ出力（詳細）
    logger.write("\n=== [Verification Evaluation] ===\n")
    logger.write(f"Confusion matrix (base vs clusters) -> {conf_csv}\n")
    logger.write(f"Assignments with prediction/flags -> {assign_csv}\n")
    logger.write(f"Per-label recall CSV -> {per_label_csv}\n")
    logger.write("\n【各ラベルの再現率（学習時の代表基本ラベルを予測として）】\n")
    for r in per_label_rows:
        logger.write(f" - {r['label']:<3}: N_base={r['N_base']:4d} Correct_base={r['Correct_base']:4d} Recall_base={0.0 if np.isnan(r['Recall_base']) else r['Recall_base']:.4f} | "
                     f"N_comp={r['N_composite']:4d} Correct_comp={r['Correct_composite']:4d} Recall_comp={0.0 if np.isnan(r['Recall_composite']) else r['Recall_composite']:.4f}\n")
    logger.write(f"\n[Summary] Macro Recall (基本ラベル)   = {macro_base:.4f}\n")
    logger.write(f"[Summary] Macro Recall (基本+応用) = {macro_comp:.4f}\n")
    logger.write("=== [Verification Evaluation End] ===\n")

    return {
        'MacroRecall_majority': macro_base,
        'MacroRecall_composite': macro_comp
    }


# =====================================================
# 3種類のbatchSOM（学習：一方式分）
# =====================================================
def run_one_method_learning(method_name, activation_distance, data_all, labels_all, times_all,
                            field_shape, lat, lon, out_dir):
    """
    学習（learning）：method_name: 'euclidean' | 'ssim' | 's1'
    activation_distance: 同上
    data_all は「空間平均を引いた偏差[hPa]」（N,D）
    """
    os.makedirs(out_dir, exist_ok=True)
    log = Logger(os.path.join(out_dir, f'{method_name}_results.log'))
    log.write(f'=== {method_name} SOM (Learning period) ===\n')
    log.write(f'Device CUDA: {torch.cuda.is_available()}, GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else "CPU"}\n')
    log.write(f'SOM size: {SOM_X} x {SOM_Y}, iter={NUM_ITER}, batch={BATCH_SIZE}, nodes_chunk={NODES_CHUNK}\n')
    log.write(f'All samples: {data_all.shape[0]}\n')
    log.write('Input representation: SLP anomaly [hPa], spatial-mean removed per sample\n')
    if len(times_all) > 0:
        tmin = pd.to_datetime(times_all.min()).strftime('%Y-%m-%d')
        tmax = pd.to_datetime(times_all.max()).strftime('%Y-%m-%d')
        log.write(f'Period: {tmin} to {tmax}\n')

    # SOM構築（3距離対応版）
    som = MultiDistMiniSom(
        x=SOM_X, y=SOM_Y, input_len=data_all.shape[1],
        sigma=2.5, learning_rate=0.5,
        neighborhood_function='gaussian',
        topology='rectangular',
        activation_distance=activation_distance,                # 'euclidean'/'ssim'/'s1'
        random_seed=SEED,
        sigma_decay='asymptotic_decay',
        s1_field_shape=field_shape,
        device='cuda' if torch.cuda.is_available() else 'cpu',
        dtype=torch.float32,
        nodes_chunk=NODES_CHUNK
    )
    som.random_weights_init(data_all)
    # σを学習全体で一方向に減衰（セグメント跨ぎで継続）
    som.set_total_iterations(NUM_ITER)

    # 固定QE評価サンプルを設定（再現性あり）
    n_eval = min(EVAL_SAMPLE_LIMIT if EVAL_SAMPLE_LIMIT else data_all.shape[0], data_all.shape[0])
    if n_eval > 0:
        rng = np.random.RandomState(SEED)
        eval_idx = rng.choice(data_all.shape[0], size=n_eval, replace=False)
        som.set_eval_indices(eval_idx)

    # ====== 学習を区切って実施し、各区切りで評価（履歴プロット用） ======
    step = max(1, NUM_ITER // SOM_EVAL_SEGMENTS)
    iter_history: Dict[str, List[float]] = {
        'iteration': [],
        'MacroRecall_majority': [],
        'MacroRecall_composite': [],
        'QuantizationError': [],
        'NodewiseMatchRate': []  # 追加：ノード代表（raw）vs medoid（raw）の一致率
    }

    current_iter = 0
    for seg in range(SOM_EVAL_SEGMENTS):
        n_it = min(step, NUM_ITER - current_iter)
        if n_it <= 0:
            break
        som.train_batch(
            data_all, num_iteration=n_it,
            batch_size=BATCH_SIZE, verbose=True, log_interval=LOG_INTERVAL,
            update_per_iteration=False, shuffle=True
        )
        current_iter += n_it

        # 量子化誤差
        qe_now = som.quantization_error(data_all, sample_limit=EVAL_SAMPLE_LIMIT, batch_size=max(32, BATCH_SIZE))

        # 評価
        winners_now = som.predict(data_all, batch_size=max(64, BATCH_SIZE))
        clusters_now = winners_to_clusters(winners_now, (SOM_X, SOM_Y))
        metrics = evaluate_clusters_only_base(
            clusters=clusters_now,
            all_labels=labels_all,
            base_labels=BASE_LABELS,
            title=f"[{method_name.upper()}] Iteration={current_iter} Evaluation (Base labels)"
        )

        # 追加: 現時点のmedoidを再計算して、match rate を算出
        node_to_medoid_idx_now, _ = compute_node_medoids_by_centroid(
            method_name=method_name,
            data_flat=data_all, winners_xy=winners_now,
            som_shape=(SOM_X, SOM_Y), field_shape=field_shape
        )
        match_rate_now, matched_nodes_now, counted_nodes_now = compute_nodewise_match_rate(
            winners_xy=winners_now,
            labels_all=labels_all,
            node_to_medoid_idx=node_to_medoid_idx_now,
            som_shape=(SOM_X, SOM_Y)
        )

        # ログ（results.log）に集約指標を追記
        log.write(f'\n[Iteration {current_iter}] QuantizationError={qe_now:.6f}\n')
        if metrics is not None:
            for k in ['MacroRecall_majority', 'MacroRecall_composite']:
                if k in metrics:
                    log.write(f'  {k} = {metrics[k]:.6f}\n')
        # 追加: match rate
        if not np.isnan(match_rate_now):
            log.write(f'  NodewiseMatchRate = {match_rate_now:.6f} (matched {matched_nodes_now}/{counted_nodes_now} nodes)\n')
        else:
            log.write(f'  NodewiseMatchRate = NaN (no countable nodes)\n')

        # 履歴に保存
        iter_history['iteration'].append(current_iter)
        iter_history['QuantizationError'].append(qe_now)
        if metrics is not None:
            for k in ['MacroRecall_majority', 'MacroRecall_composite']:
                iter_history[k].append(metrics.get(k, np.nan))
        else:
            for k in ['MacroRecall_majority', 'MacroRecall_composite']:
                iter_history[k].append(np.nan)
        iter_history['NodewiseMatchRate'].append(match_rate_now if not np.isnan(match_rate_now) else np.nan)

    # イテレーション履歴の保存（CSV/PNG）
    iter_csv = os.path.join(out_dir, f'{method_name}_iteration_metrics.csv')
    save_metrics_history_to_csv(iter_history, iter_csv)
    iter_png = os.path.join(out_dir, f'{method_name}_iteration_vs_metrics.png')
    plot_iteration_metrics(iter_history, iter_png)
    # 追加：各メトリクスの単独画像
    plot_iteration_metrics_single(iter_history, out_dir, filename_prefix=method_name)

    log.write(f'\nIteration metrics saved: CSV={iter_csv}, PNG={iter_png} and per-metric PNGs\n')

    # ====== 最終モデルでの割当 ======
    winners_all = som.predict(data_all, batch_size=max(64, BATCH_SIZE))

    # 割当CSV
    assign_csv_all = os.path.join(out_dir, f'{method_name}_assign_all.csv')
    pd.DataFrame({
        'time': times_all,
        'bmu_x': winners_all[:,0], 'bmu_y': winners_all[:,1],
        'label_raw': labels_all if labels_all is not None else ['']*len(winners_all)
    }).to_csv(assign_csv_all, index=False, encoding='utf-8-sig')
    log.write(f'\nAssigned BMU (all) -> {assign_csv_all}\n')

    # ノード平均パターン（偏差[hPa]）
    bigmap_all = os.path.join(out_dir, f'{method_name}_som_node_avg_all.png')
    plot_som_node_average_patterns(
        data_all, winners_all, lat, lon, (SOM_X,SOM_Y),
        save_path=bigmap_all,
        title=f'{method_name.upper()} SOM Node Avg SLP Anomaly (All)'
    )
    log.write(f'Node average patterns (all) -> {bigmap_all}\n')

    # 各ノード平均画像（個別）
    pernode_dir_all = os.path.join(out_dir, f'{method_name}_pernode_all')
    save_each_node_mean_image(data_all, winners_all, lat, lon, (SOM_X,SOM_Y),
                              out_dir=pernode_dir_all, prefix='all')
    log.write(f'Per-node mean images (all) -> {pernode_dir_all}\n')

    # ===== ノードの多数決（元ラベル/基本ラベル） =====
    node_to_majority_raw, node_to_majority_base = compute_training_node_majorities(
        winners_all, labels_all, BASE_LABELS, (SOM_X, SOM_Y)
    )

    # ===== ノードmedoid（重心に最も近いサンプル） =====
    node_to_medoid_idx, node_to_medoid_dist = compute_node_medoids_by_centroid(
        method_name=method_name,
        data_flat=data_all, winners_xy=winners_all,
        som_shape=(SOM_X, SOM_Y), field_shape=field_shape
    )

    # ○×可視化（背景色で一致/不一致を表現、日付も表示）
    medoid_bigmap = os.path.join(out_dir, f'{method_name}_som_node_medoid_all.png')
    plot_som_node_medoid_patterns(
        data_flat=data_all,
        node_to_medoid_idx=node_to_medoid_idx,
        lat=lat, lon=lon, som_shape=(SOM_X, SOM_Y),
        times_all=times_all,
        save_path=medoid_bigmap,
        title=f'{method_name.upper()} SOM Node Medoid (closest-to-centroid)'
    )
    log.write(f'Node medoid map (all) -> {medoid_bigmap}\n')

    # 各ノードmedoid個別図
    pernode_medoid_dir = os.path.join(out_dir, f'{method_name}_pernode_medoid_all')
    save_each_node_medoid_image(
        data_flat=data_all,
        node_to_medoid_idx=node_to_medoid_idx,
        lat=lat, lon=lon, som_shape=(SOM_X, SOM_Y),
        out_dir=pernode_medoid_dir, prefix='all'
    )
    log.write(f'Per-node medoid images (all) -> {pernode_medoid_dir}\n')

    # ===== True Medoid（総距離最小）も計算・保存 =====
    node_to_true_medoid_idx, node_to_true_medoid_avgdist = compute_node_true_medoids(
        method_name=method_name,
        data_flat=data_all, winners_xy=winners_all,
        som_shape=(SOM_X, SOM_Y), field_shape=field_shape,
        fusion_alpha=0.5
    )

    # True Medoid マップ
    true_medoid_bigmap = os.path.join(out_dir, f'{method_name}_som_node_true_medoid_all.png')
    plot_som_node_medoid_patterns(
        data_flat=data_all,
        node_to_medoid_idx=node_to_true_medoid_idx,
        lat=lat, lon=lon, som_shape=(SOM_X, SOM_Y),
        times_all=times_all,
        save_path=true_medoid_bigmap,
        title=f'{method_name.upper()} SOM Node True Medoid (min-sum-of-distances)'
    )
    log.write(f'True medoid map (all) -> {true_medoid_bigmap}\n')

    # True Medoid 個別図
    pernode_true_medoid_dir = os.path.join(out_dir, f'{method_name}_pernode_true_medoid_all')
    save_each_node_medoid_image(
        data_flat=data_all,
        node_to_medoid_idx=node_to_true_medoid_idx,
        lat=lat, lon=lon, som_shape=(SOM_X, SOM_Y),
        out_dir=pernode_true_medoid_dir, prefix='all_true'
    )
    log.write(f'Per-node true medoid images (all) -> {pernode_true_medoid_dir}\n')

    # True Medoid CSV
    rows_true = []
    for (ix, iy), mi in sorted(node_to_true_medoid_idx.items()):
        t_str = format_date_yyyymmdd(times_all[mi]) if len(times_all)>0 else ''
        raw = labels_all[mi] if labels_all is not None else ''
        label_base_or_none = basic_label_or_none(raw, BASE_LABELS)
        avgdist = node_to_true_medoid_avgdist.get((ix,iy), np.nan)
        rows_true.append({
            'node_x': ix, 'node_y': iy, 'node_flat': ix*SOM_Y+iy,
            'true_medoid_index': mi, 'time': t_str,
            'label_raw': raw,
            'label': label_base_or_none,
            'avg_distance_in_node': avgdist
        })
    true_medoid_csv = os.path.join(out_dir, f'{method_name}_node_true_medoids.csv')
    pd.DataFrame(rows_true).to_csv(true_medoid_csv, index=False, encoding='utf-8-sig')
    log.write(f'Node true medoid CSV -> {true_medoid_csv}\n')

    # medoid選定結果のCSV（labelは基本ラベル or None、rawは元ラベル）
    rows = []
    for (ix, iy), mi in sorted(node_to_medoid_idx.items()):
        t_str = format_date_yyyymmdd(times_all[mi]) if len(times_all)>0 else ''
        raw = labels_all[mi] if labels_all is not None else ''
        label_base_or_none = basic_label_or_none(raw, BASE_LABELS)
        dist = node_to_medoid_dist.get((ix,iy), np.nan)
        majority_raw = node_to_majority_raw.get((ix,iy), None)
        match = (raw == majority_raw) if (raw is not None and majority_raw is not None) else False
        rows.append({
            'node_x': ix, 'node_y': iy, 'node_flat': ix*SOM_Y+iy,
            'medoid_index': mi, 'time': t_str,
            'label_raw': raw,           # 元ラベル（複合含む）
            'label': label_base_or_none,  # 基本ラベル or None（複合なら None）
            'majority_label': majority_raw,      # 互換性維持：ノードの代表ラベル（元ラベル）
            'representative_label': majority_raw, # 別名（代表ラベル）も追加
            'match': match,
            'distance': dist
        })
    medoid_csv = os.path.join(out_dir, f'{method_name}_node_medoids.csv')
    pd.DataFrame(rows).to_csv(medoid_csv, index=False, encoding='utf-8-sig')
    log.write(f'Node medoid selection CSV -> {medoid_csv}\n')

    # ログに概要（全ノード表示）: label_raw と 代表ラベルを出力（複合ラベルも可視化）
    log.write('\n[Node medoid summary]\n')
    for r in rows:
        log.write(
            f"({r['node_x']},{r['node_y']}): idx={r['medoid_index']}, "
            f"time={r['time']}, label_raw={r['label_raw']}, rep_label={r['representative_label']}, "
            f"match={r['match']}, dist={r['distance']:.4f}\n"
        )

    # ===== 「SOM Node-wise Analysis」の一致可視化（背景色） =====
    nodewise_vis_path = os.path.join(out_dir, f'{method_name}_nodewise_analysis_match.png')
    plot_nodewise_match_map(
        winners_xy=winners_all,
        labels_all=labels_all,
        node_to_medoid_idx=node_to_medoid_idx,
        times_all=times_all,
        som_shape=(SOM_X, SOM_Y),
        save_path=nodewise_vis_path,
        title=f'{method_name.upper()} SOM Node-wise Analysis (background: green=match / red=not)'
    )
    log.write(f'Node-wise match visualization -> {nodewise_vis_path}\n')

    # ===== 評価（ラベルがあれば） =====
    final_match_rate, final_matched_nodes, final_counted_nodes = compute_nodewise_match_rate(
        winners_xy=winners_all,
        labels_all=labels_all,
        node_to_medoid_idx=node_to_medoid_idx,
        som_shape=(SOM_X, SOM_Y)
    )

    if labels_all is not None:
        # 混同行列（基本ラベルのみ）を構築・保存
        clusters_all = winners_to_clusters(winners_all, (SOM_X, SOM_Y))
        cm, cluster_names = build_confusion_matrix_only_base(clusters_all, labels_all, BASE_LABELS)
        conf_csv = os.path.join(out_dir, f'{method_name}_confusion_matrix_all.csv')
        cm.to_csv(conf_csv, encoding='utf-8-sig')
        log.write(f'\nConfusion matrix (base vs clusters) -> {conf_csv}\n')

        # 集約指標（基本/複合）
        metrics = evaluate_clusters_only_base(
            clusters=clusters_all,
            all_labels=labels_all,
            base_labels=BASE_LABELS,
            title=f"[{method_name.upper()}] SOM Final Evaluation (Base labels)"
        )
        if metrics is not None:
            log.write('\n[Final Metrics]\n')
            for k in ['MacroRecall_majority', 'MacroRecall_composite']:
                if k in metrics:
                    log.write(f'  {k} = {metrics[k]:.6f}\n')
            # 追加：match rate
            if not np.isnan(final_match_rate):
                log.write(f'  NodewiseMatchRate = {final_match_rate:.6f} (matched {final_matched_nodes}/{final_counted_nodes} nodes)\n')
            else:
                log.write(f'  NodewiseMatchRate = NaN (no countable nodes)\n')

        # ラベル分布ヒートマップ（基本ラベルのみ） 1枚
        dist_dir_all = os.path.join(out_dir, f'{method_name}_label_dist_all')
        plot_label_distributions_base(winners_all, labels_all, BASE_LABELS, (SOM_X,SOM_Y), dist_dir_all, title_prefix='All')
        # 追加：基本ラベルごとの個別画像
        save_label_distributions_base_individual(winners_all, labels_all, BASE_LABELS, (SOM_X,SOM_Y), dist_dir_all, title_prefix='All')

        log.write(f'Label-distribution heatmaps (base only) -> {dist_dir_all}\n')

        # ノード詳細（構成・月分布など）を results.log に（代表ラベルrawも出力）
        analyze_nodes_detail_to_log(
            clusters_all, labels_all, times_all, BASE_LABELS, (SOM_X, SOM_Y),
            log, title=f'[{method_name.upper()}] SOM Node-wise Analysis'
        )

        # 代表ノード群ベースの再現率（基本/複合）と、各ラベルの代表ノード一覧を results.log に出力
        log_som_recall_by_label_with_nodes(
            log=log,
            winners_xy=winners_all,
            labels_all=labels_all,
            base_labels=BASE_LABELS,
            som_shape=(SOM_X, SOM_Y),
            section_title='SOM代表ノード群ベースの再現率（基本/複合）'
        )
    else:
        # ラベルが無いケースでも match rate は NaN になるだけ
        log.write('Labels not found; skip evaluation.\n')
        if not np.isnan(final_match_rate):
            log.write('\n[Final Metrics]\n')
            log.write(f'  NodewiseMatchRate = {final_match_rate:.6f} (matched {final_matched_nodes}/{final_counted_nodes} nodes)\n')

    # 学習時ノード代表（raw/基本）ラベルをJSONに保存（検証時に利用）
    majority_rows = []
    for ix in range(SOM_X):
        for iy in range(SOM_Y):
            majority_rows.append({
                'node_x': ix, 'node_y': iy,
                'majority_raw': node_to_majority_raw.get((ix,iy)),
                'majority_base': node_to_majority_base.get((ix,iy))
            })
    maj_json = os.path.join(out_dir, 'node_majorities.json')
    with open(maj_json, 'w', encoding='utf-8') as f:
        json.dump(majority_rows, f, ensure_ascii=False, indent=2)
    log.write(f'Node majorities (raw/base) saved -> {maj_json}\n')

    log.write('\n=== Done (learning) ===\n')
    log.close()

    # 検証で使用するためにSOMインスタンスと学習代表（raw/基本）辞書を返却
    return som, node_to_majority_raw, node_to_majority_base


# =====================================================
# 検証（学習済モデルを使って、検証期間データで評価）
# =====================================================
def run_one_method_verification(method_name: str,
                                som: MultiDistMiniSom,
                                train_majority_raw: Dict[Tuple[int,int], Optional[str]],
                                train_majority_base: Dict[Tuple[int,int], Optional[str]],
                                data_valid: np.ndarray,
                                labels_valid: Optional[List[Optional[str]]],
                                times_valid: np.ndarray,
                                field_shape: Tuple[int,int],
                                lat: np.ndarray, lon: np.ndarray,
                                out_dir: str):
    """
    学習済み SOM を用いて検証データをBMU割当し、学習時のノード代表（基本）ラベルで検証を評価
    """
    os.makedirs(out_dir, exist_ok=True)
    vlog = Logger(os.path.join(out_dir, f'{method_name}_verification.log'))
    vlog.write(f'=== {method_name} SOM (Verification period) ===\n')
    vlog.write(f'Verification samples: {data_valid.shape[0]}\n')
    if len(times_valid) > 0:
        tmin = pd.to_datetime(times_valid.min()).strftime('%Y-%m-%d')
        tmax = pd.to_datetime(times_valid.max()).strftime('%Y-%m-%d')
        vlog.write(f'Verification Period: {tmin} to {tmax}\n')

    # 検証データのBMU予測
    winners_val = som.predict(data_valid, batch_size=max(64, BATCH_SIZE))

    # ラベルがあれば、検証評価（学習代表基本ラベルを予測とする）
    if labels_valid is not None and len(labels_valid) == len(winners_val):
        # 混同行列や per-label 再現率など
        metrics = evaluate_verification_with_training_majority(
            winners_xy_valid=winners_val,
            labels_valid=labels_valid,
            times_valid=times_valid,
            base_labels=BASE_LABELS,
            som_shape=(SOM_X, SOM_Y),
            node_to_majority_base_train=train_majority_base,
            out_dir=out_dir,
            method_name=method_name,
            logger=vlog
        )
        # ラベル分布ヒートマップ（基本/個別）
        dist_dir_val = os.path.join(out_dir, f'{method_name}_verification_label_dist')
        plot_label_distributions_base(winners_val, labels_valid, BASE_LABELS, (SOM_X, SOM_Y), dist_dir_val, title_prefix='Verification')
        save_label_distributions_base_individual(winners_val, labels_valid, BASE_LABELS, (SOM_X, SOM_Y), dist_dir_val, title_prefix='Verification')
        vlog.write(f'Label-distribution heatmaps for verification -> {dist_dir_val}\n')
    else:
        vlog.write('Labels not found for verification; skip evaluation.\n')

    # 参考：検証データのノード平均（必要であれば）
    avg_map_path = os.path.join(out_dir, f'{method_name}_verification_node_avg.png')
    plot_som_node_average_patterns(
        data_valid, winners_val, lat, lon, (SOM_X,SOM_Y),
        save_path=avg_map_path,
        title=f'{method_name.upper()} SOM Node Avg SLP Anomaly (Verification)'
    )
    vlog.write(f'Verification node average patterns -> {avg_map_path}\n')

    vlog.write('=== Done (verification) ===\n')
    vlog.close()


# =====================================================
# メイン
# =====================================================
def main():
    set_reproducibility(SEED)
    setup_logging_v4()
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    logging.info(f"使用デバイス: {device.upper()}")
    logging.info(f"SOM(3type): size={SOM_X}x{SOM_Y}, iters={NUM_ITER}, batch={BATCH_SIZE}, nodes_chunk={NODES_CHUNK}")

    # 学習データ（1991-01-01〜1999-12-31）
    X_for_s1_L, X_original_hpa_L, X_anomaly_hpa_L, lat, lon, d_lat, d_lon, ts_L, labels_L = load_and_prepare_data_unified(
        DATA_FILE, LEARN_START, LEARN_END, device
    )
    data_learn = X_anomaly_hpa_L.reshape(X_anomaly_hpa_L.shape[0], -1).astype(np.float32)

    # 検証データ（2000-01-01〜2000-12-31）
    X_for_s1_V, X_original_hpa_V, X_anomaly_hpa_V, lat2, lon2, d_lat2, d_lon2, ts_V, labels_V = load_and_prepare_data_unified(
        DATA_FILE, VALID_START, VALID_END, device
    )
    data_valid = X_anomaly_hpa_V.reshape(X_anomaly_hpa_V.shape[0], -1).astype(np.float32)

    # 次元・座標は同じはずだが、保険で確認
    assert d_lat == d_lat2 and d_lon == d_lon2, "学習/検証でグリッドサイズが異なります。"
    assert np.allclose(lat, lat2) and np.allclose(lon, lon2), "学習/検証でlat/lonが異なります。"

    field_shape = (d_lat, d_lon)

    # 3種類のbatchSOM（学習→検証）
    print(f"PyTorch version: {torch.__version__}")
    print(f"CUDA available: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"GPU: {torch.cuda.get_device_name(0)}")

    methods = [
        ('euclidean', 'euclidean'),
        ('ssim',      'ssim'),
        ('ssim5',     'ssim5'),    # 論文仕様（5x5窓・C=0）版SSIM
        ('s1',        's1'),
        ('s1ssim',    's1ssim')    # S1とSSIM(5x5)の融合
    ]
    for mname, adist in methods:
        # 学習
        out_dir_learn = os.path.join(LEARNING_ROOT, f'{mname}_som')
        som, majority_raw_train, majority_base_train = run_one_method_learning(
            method_name=mname, activation_distance=adist,
            data_all=data_learn, labels_all=labels_L, times_all=ts_L,
            field_shape=field_shape, lat=lat, lon=lon, out_dir=out_dir_learn
        )
        # 検証
        out_dir_verif = os.path.join(VERIF_ROOT, f'{mname}_som')
        run_one_method_verification(
            method_name=mname,
            som=som,
            train_majority_raw=majority_raw_train,
            train_majority_base=majority_base_train,
            data_valid=data_valid,
            labels_valid=labels_V,
            times_valid=ts_V,
            field_shape=field_shape,
            lat=lat, lon=lon,
            out_dir=out_dir_verif
        )

    logging.info("全処理完了（学習＋検証）。")


if __name__ == '__main__':
    main()

```

/Users/takumi0616/Develop/docker_miniconda/src/PressurePattern/minisom.py
```
import math
import numpy as np
from typing import Tuple, Optional, List

import torch
from torch import Tensor
import torch.nn.functional as F

try:
    from tqdm import trange
except Exception:
    def trange(n, **kwargs):
        return range(n)


def _to_device(x: np.ndarray, device: torch.device, dtype=torch.float32) -> Tensor:
    return torch.as_tensor(x, device=device, dtype=dtype)


def _as_numpy(x: Tensor) -> np.ndarray:
    return x.detach().cpu().numpy()


class MiniSom:
    """
    PyTorch GPU版 SOM（batchSOM）
      - activation_distance:
          'euclidean'（ユークリッド）
          'ssim'     （全体1窓の簡略SSIM）
          'ssim5'    （論文仕様に近い 5x5 窓・C=0 のSSIM：移動窓平均）
          's1'       （Teweles–Wobus S1）
          's1ssim'   （S1とSSIM(5x5)の融合距離：サンプル毎min-max正規化後の等重み和）
      - 学習は「ミニバッチ版バッチSOM」：BMU→近傍重み→分子/分母累積→一括更新
      - 全ての重い計算はGPU実行
      - σ（近傍幅）は学習全体で一方向に減衰させる（セグメント学習でも継続）
      - 任意頻度で“距離一貫性”のためのメドイド置換（ノード重みを最近傍サンプルへ置換）を実行可
    """
    def __init__(self,
                 x: int,
                 y: int,
                 input_len: int,
                 sigma: float = 1.0,
                 learning_rate: float = 0.5,
                 neighborhood_function: str = 'gaussian',
                 topology: str = 'rectangular',
                 activation_distance: str = 's1',
                 random_seed: Optional[int] = None,
                 sigma_decay: str = 'asymptotic_decay',
                 s1_field_shape: Optional[Tuple[int, int]] = None,
                 device: Optional[str] = None,
                 dtype: torch.dtype = torch.float32,
                 nodes_chunk: int = 16):
        if device is None:
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
        self.device = torch.device(device)
        self.dtype = dtype

        if random_seed is not None:
            torch.manual_seed(random_seed)
            np.random.seed(random_seed)

        self.x = x
        self.y = y
        self.m = x * y
        self.input_len = input_len
        self.sigma0 = float(sigma)
        self.learning_rate = float(learning_rate)
        self.neighborhood_function = 'gaussian'
        self.topology = topology
        self.sigma_decay = sigma_decay
        self.nodes_chunk = int(nodes_chunk)

        # 学習全体の反復管理（σ継続減衰用）
        self.global_iter: int = 0
        self.total_iters: Optional[int] = None

        # メドイド置換頻度（None: 不使用, k: k反復ごと）
        self.medoid_replace_every: Optional[int] = None

        # 評価用の固定サンプルインデックス（QEを安定化）
        self.eval_indices: Optional[Tensor] = None

        # 距離タイプ
        activation_distance = activation_distance.lower()
        if activation_distance not in ('s1', 'euclidean', 'ssim', 'ssim5', 's1ssim'):
            raise ValueError('activation_distance must be one of "s1","euclidean","ssim","ssim5","s1ssim"')
        self.activation_distance = activation_distance

        # 画像形状
        if s1_field_shape is None:
            raise ValueError('s1_field_shape=(H,W) is required for all distances in this implementation.')
        if s1_field_shape[0] * s1_field_shape[1] != input_len:
            raise ValueError(f's1_field_shape={s1_field_shape} does not match input_len={input_len}.')
        self.field_shape = s1_field_shape
        H, W = s1_field_shape

        # グリッド座標
        gx, gy = torch.meshgrid(torch.arange(x), torch.arange(y), indexing='ij')
        self.grid_coords = torch.stack([gx.flatten(), gy.flatten()], dim=1).to(self.device, torch.float32)

        # 重み（(m,H,W)）
        self.weights = (torch.rand((self.m, H, W), device=self.device, dtype=self.dtype) * 2 - 1)

        if self.neighborhood_function != 'gaussian':
            self.neighborhood_function = 'gaussian'

        # SSIM定数（簡略SSIM用：安定化）
        self.c1 = 1e-8
        self.c2 = 1e-8

        # 5x5移動窓SSIM用カーネル（平均フィルタ）
        self._kernel5: Optional[Tensor] = None
        self._win5_size: int = 5
        self._win5_pad: int = 2

    # ---------- 外部制御 ----------
    def set_total_iterations(self, total_iters: int):
        """学習全体の反復回数を設定（σ減衰の基準）。複数回train_batchを呼ぶ前に設定してください。"""
        self.total_iters = int(total_iters)

    def set_medoid_replace_every(self, k: Optional[int]):
        """k反復ごとにメドイド置換（各ノード重みを距離的に最も近いサンプルへ置換）を行う。Noneまたは0で無効。"""
        if k is None or k <= 0:
            self.medoid_replace_every = None
        else:
            self.medoid_replace_every = int(k)

    def set_eval_indices(self, idx: Optional[np.ndarray]):
        """
        評価（quantization_error/predict等の固定評価で使用）用のインデックスを設定。
        Noneで解除。idxはデータ配列に対する行インデックス。
        """
        if idx is None:
            self.eval_indices = None
        else:
            self.eval_indices = torch.as_tensor(idx, device=self.device, dtype=torch.long)

    # ---------- ユーティリティ ----------
    def get_weights(self) -> np.ndarray:
        H, W = self.field_shape
        w_flat = self.weights.reshape(self.m, H * W)
        w_grid = w_flat.reshape(self.x, self.y, H * W)
        return _as_numpy(w_grid)

    def random_weights_init(self, data: np.ndarray):
        H, W = self.field_shape
        n = data.shape[0]
        if n < self.m:
            idx = np.random.choice(n, self.m, replace=True)
        else:
            idx = np.random.choice(n, self.m, replace=False)
        w0 = data[idx].reshape(self.m, H, W)
        self.weights = _to_device(w0, self.device, self.dtype).clone()

    # ---------- スケジューラ ----------
    def _sigma_at_val(self, t: int, max_iter: int) -> float:
        if self.sigma_decay == 'asymptotic_decay':
            return self.sigma0 / (1 + t / (max_iter / 2.0))
        elif self.sigma_decay == 'linear_decay':
            return max(1e-3, self.sigma0 * (1 - t / max_iter))
        else:
            return self.sigma0 / (1 + t / (max_iter / 2.0))

    # ---------- 近傍関数 ----------
    @torch.no_grad()
    def _neighborhood(self, bmu_flat: Tensor, sigma: float) -> Tensor:
        bmu_xy = self.grid_coords[bmu_flat]  # (B,2)
        d2 = ((bmu_xy.unsqueeze(1) - self.grid_coords.unsqueeze(0)) ** 2).sum(dim=-1)  # (B,m)
        h = torch.exp(-d2 / (2 * (sigma ** 2) + 1e-9))
        return h

    # ---------- 距離計算（バッチ→全ノード） ----------
    @torch.no_grad()
    def _euclidean_distance_batch(self, Xb: Tensor) -> Tensor:
        """
        Xb: (B,H,W) -> 距離 (B,m)
        d^2 = sum((X-W)^2), 戻りは sqrt(d^2)（単調変換）
        """
        B, H, W = Xb.shape
        Xf = Xb.reshape(B, -1)                # (B,D)
        Wf = self.weights.reshape(self.m, -1) # (m,D)
        x2 = (Xf * Xf).sum(dim=1, keepdim=True)         # (B,1)
        w2 = (Wf * Wf).sum(dim=1, keepdim=True).T       # (1,m)
        cross = Xf @ Wf.T                                # (B,m)
        d2 = x2 + w2 - 2 * cross
        d2 = torch.clamp(d2, min=0.0)
        return torch.sqrt(d2 + 1e-12)

    @torch.no_grad()
    def _ssim_distance_batch(self, Xb: Tensor, nodes_chunk: Optional[int] = None) -> Tensor:
        """
        簡略SSIM（全体1窓）
        Xb: (B,H,W)
        戻り: (B,m) の "距離" = 1 - SSIM
        """
        if nodes_chunk is None:
            nodes_chunk = self.nodes_chunk
        B, H, W = Xb.shape
        out = torch.empty((B, self.m), device=Xb.device, dtype=self.dtype)

        mu_x = Xb.mean(dim=(1, 2))                  # (B,)
        Xc = Xb - mu_x.view(B, 1, 1)
        var_x = (Xc * Xc).mean(dim=(1, 2))          # (B,)

        for start in range(0, self.m, nodes_chunk):
            end = min(start + nodes_chunk, self.m)
            Wc = self.weights[start:end]                       # (Mc,H,W)
            mu_w = Wc.mean(dim=(1, 2))                         # (Mc,)
            Wc2 = Wc - mu_w.view(-1, 1, 1)
            var_w = (Wc2 * Wc2).mean(dim=(1, 2))               # (Mc,)

            cov = (Xc.unsqueeze(1) * Wc2.unsqueeze(0)).mean(dim=(2, 3))  # (B,Mc)

            l_num = (2 * mu_x.view(B, 1) * mu_w.view(1, -1) + self.c1)
            l_den = (mu_x.view(B, 1) ** 2 + mu_w.view(1, -1) ** 2 + self.c1)
            c_num = (2 * cov + self.c2)
            c_den = (var_x.view(B, 1) + var_w.view(1, -1) + self.c2)
            ssim = (l_num * c_num) / (l_den * c_den + 1e-12)
            dist = 1.0 - ssim
            out[:, start:end] = dist

        return out

    @torch.no_grad()
    def _ensure_kernel5(self):
        if self._kernel5 is None:
            k = torch.ones((1, 1, self._win5_size, self._win5_size), device=self.device, dtype=self.dtype) / float(self._win5_size * self._win5_size)
            self._kernel5 = k

    @torch.no_grad()
    def _ssim5_distance_batch(self, Xb: Tensor, nodes_chunk: Optional[int] = None) -> Tensor:
        """
        論文仕様に近いSSIM: 5x5移動窓・C=0（分母のみ数値安定化）
        Xb: (B,H,W)
        戻り: (B,m) の "距離" = 1 - mean(SSIM_map)
        """
        if nodes_chunk is None:
            nodes_chunk = self.nodes_chunk
        self._ensure_kernel5()
        eps = 1e-12
        B, H, W = Xb.shape
        out = torch.empty((B, self.m), device=Xb.device, dtype=self.dtype)

        # X 側のローカル統計
        X = Xb.unsqueeze(1)  # (B,1,H,W)
        X_pad = F.pad(X, (self._win5_pad, self._win5_pad, self._win5_pad, self._win5_pad), mode='reflect')
        mu_x = F.conv2d(X_pad, self._kernel5, padding=0)                      # (B,1,H,W)
        mu_x2 = F.conv2d(X_pad * X_pad, self._kernel5, padding=0)             # (B,1,H,W)
        var_x = torch.clamp(mu_x2 - mu_x * mu_x, min=0.0)                     # (B,1,H,W)

        for start in range(0, self.m, nodes_chunk):
            end = min(start + nodes_chunk, self.m)
            Wc = self.weights[start:end].unsqueeze(1)                          # (Mc,1,H,W)
            W_pad = F.pad(Wc, (self._win5_pad, self._win5_pad, self._win5_pad, self._win5_pad), mode='reflect')
            mu_w = F.conv2d(W_pad, self._kernel5, padding=0)                  # (Mc,1,H,W)
            mu_w2 = F.conv2d(W_pad * W_pad, self._kernel5, padding=0)         # (Mc,1,H,W)
            var_w = torch.clamp(mu_w2 - mu_w * mu_w, min=0.0)                 # (Mc,1,H,W)

            # 共分散: mean(x*w) - mu_x*mu_w
            prod = (X.unsqueeze(1) * Wc.unsqueeze(0)).reshape(B * (end - start), 1, H, W)  # (B*Mc,1,H,W)
            prod_pad = F.pad(prod, (self._win5_pad, self._win5_pad, self._win5_pad, self._win5_pad), mode='reflect')
            mu_xw = F.conv2d(prod_pad, self._kernel5, padding=0).reshape(B, end - start, 1, H, W)  # (B,Mc,1,H,W)

            mu_x_b = mu_x.unsqueeze(1)                         # (B,1,1,H,W)
            mu_w_mc = mu_w.unsqueeze(0)                        # (1,Mc,1,H,W)
            var_x_b = var_x.unsqueeze(1)                       # (B,1,1,H,W)
            var_w_mc = var_w.unsqueeze(0)                      # (1,Mc,1,H,W)
            cov = mu_xw - (mu_x_b * mu_w_mc)                   # (B,Mc,1,H,W)

            # SSIMマップ（C1=C2=0だが分母にのみepsガード）
            l_num = 2 * (mu_x_b * mu_w_mc)
            l_den = (mu_x_b * mu_x_b + mu_w_mc * mu_w_mc)
            c_num = 2 * cov
            c_den = (var_x_b + var_w_mc)
            ssim_map = (l_num * c_num) / (l_den * c_den + eps)               # (B,Mc,1,H,W)

            # 空間平均
            ssim_avg = ssim_map.mean(dim=(2, 3, 4))                          # (B,Mc)
            out[:, start:end] = 1.0 - ssim_avg

        return out

    @torch.no_grad()
    def _s1_distance_batch(self, Xb: Tensor, nodes_chunk: Optional[int] = None) -> Tensor:
        """
        Xb: (B,H,W) 戻り (B,m) S1距離
        """
        if nodes_chunk is None:
            nodes_chunk = self.nodes_chunk
        B, H, W = Xb.shape
        dXdx = Xb[:, :, 1:] - Xb[:, :, :-1]  # (B,H,W-1)
        dXdy = Xb[:, 1:, :] - Xb[:, :-1, :]  # (B,H-1,W)

        out = torch.empty((B, self.m), device=Xb.device, dtype=self.dtype)
        dWdx_full = self.weights[:, :, 1:] - self.weights[:, :, :-1]
        dWdy_full = self.weights[:, 1:, :] - self.weights[:, :-1, :]

        for start in range(0, self.m, nodes_chunk):
            end = min(start + nodes_chunk, self.m)
            dWdx = dWdx_full[start:end]   # (Mc,H,W-1)
            dWdy = dWdy_full[start:end]   # (Mc,H-1,W)
            num_dx = (torch.abs(dWdx.unsqueeze(0) - dXdx.unsqueeze(1))).sum(dim=(2, 3))
            num_dy = (torch.abs(dWdy.unsqueeze(0) - dXdy.unsqueeze(1))).sum(dim=(2, 3))
            num = num_dx + num_dy

            den_dx = (torch.maximum(torch.abs(dWdx).unsqueeze(0), torch.abs(dXdx).unsqueeze(1))).sum(dim=(2, 3))
            den_dy = (torch.maximum(torch.abs(dWdy).unsqueeze(0), torch.abs(dXdy).unsqueeze(1))).sum(dim=(2, 3))
            denom = den_dx + den_dy
            s1 = 100.0 * num / (denom + 1e-12)
            out[:, start:end] = s1

        return out

    @torch.no_grad()
    def _distance_batch(self, Xb: Tensor, nodes_chunk: Optional[int] = None) -> Tensor:
        if self.activation_distance == 's1':
            return self._s1_distance_batch(Xb, nodes_chunk=nodes_chunk)
        elif self.activation_distance == 'euclidean':
            return self._euclidean_distance_batch(Xb)
        elif self.activation_distance == 'ssim':
            return self._ssim_distance_batch(Xb, nodes_chunk=nodes_chunk)
        elif self.activation_distance == 'ssim5':
            return self._ssim5_distance_batch(Xb, nodes_chunk=nodes_chunk)
        elif self.activation_distance == 's1ssim':
            # 融合：各サンプル毎にノード方向min-max正規化して等重み和
            d1 = self._s1_distance_batch(Xb, nodes_chunk=nodes_chunk)    # (B,m)
            d2 = self._ssim5_distance_batch(Xb, nodes_chunk=nodes_chunk) # (B,m)
            min1 = d1.min(dim=1, keepdim=True).values
            max1 = d1.max(dim=1, keepdim=True).values
            min2 = d2.min(dim=1, keepdim=True).values
            max2 = d2.max(dim=1, keepdim=True).values
            dn1 = (d1 - min1) / (max1 - min1 + 1e-12)
            dn2 = (d2 - min2) / (max2 - min2 + 1e-12)
            return 0.5 * (dn1 + dn2)
        else:
            raise RuntimeError('Unknown activation_distance')

    @torch.no_grad()
    def bmu_indices(self, Xb: Tensor, nodes_chunk: Optional[int] = None) -> Tensor:
        dists = self._distance_batch(Xb, nodes_chunk=nodes_chunk)
        bmu = torch.argmin(dists, dim=1)
        return bmu

    # ---------- 距離計算（バッチ→単一参照：メドイド置換等で使用） ----------
    @torch.no_grad()
    def _euclidean_to_ref(self, Xb: Tensor, ref: Tensor) -> Tensor:
        # Xb: (B,H,W), ref: (H,W) -> (B,)
        diff = Xb - ref.view(1, *ref.shape)
        d2 = (diff * diff).sum(dim=(1, 2))
        return torch.sqrt(d2 + 1e-12)

    @torch.no_grad()
    def _ssim_global_to_ref(self, Xb: Tensor, ref: Tensor) -> Tensor:
        # 1 - SSIM（全体1窓）
        B, H, W = Xb.shape
        mu_x = Xb.mean(dim=(1, 2))              # (B,)
        xc = Xb - mu_x.view(B, 1, 1)
        var_x = (xc * xc).mean(dim=(1, 2))      # (B,)
        mu_r = ref.mean()
        rc = ref - mu_r
        var_r = (rc * rc).mean()
        cov = (xc * rc.view(1, H, W)).mean(dim=(1, 2))  # (B,)
        l_num = (2 * mu_x * mu_r + self.c1)
        l_den = (mu_x ** 2 + mu_r ** 2 + self.c1)
        c_num = (2 * cov + self.c2)
        c_den = (var_x + var_r + self.c2)
        ssim = (l_num * c_num) / (l_den * c_den + 1e-12)
        return 1.0 - ssim

    @torch.no_grad()
    def _ssim5_to_ref(self, Xb: Tensor, ref: Tensor) -> Tensor:
        # 1 - mean(SSIM_map(5x5, C=0)) 対参照
        self._ensure_kernel5()
        eps = 1e-12
        B, H, W = Xb.shape
        X = Xb.unsqueeze(1)
        R = ref.view(1, 1, H, W)

        X_pad = F.pad(X, (self._win5_pad, self._win5_pad, self._win5_pad, self._win5_pad), mode='reflect')
        R_pad = F.pad(R, (self._win5_pad, self._win5_pad, self._win5_pad, self._win5_pad), mode='reflect')

        mu_x = F.conv2d(X_pad, self._kernel5, padding=0)                 # (B,1,H,W)
        mu_r = F.conv2d(R_pad, self._kernel5, padding=0)                 # (1,1,H,W)

        mu_x2 = F.conv2d(X_pad * X_pad, self._kernel5, padding=0)
        mu_r2 = F.conv2d(R_pad * R_pad, self._kernel5, padding=0)
        var_x = torch.clamp(mu_x2 - mu_x * mu_x, min=0.0)
        var_r = torch.clamp(mu_r2 - mu_r * mu_r, min=0.0)

        mu_xr = F.conv2d(F.pad(X * R, (self._win5_pad, self._win5_pad, self._win5_pad, self._win5_pad), mode='reflect'),
                         self._kernel5, padding=0)
        cov = mu_xr - mu_x * mu_r

        l_num = 2 * (mu_x * mu_r)
        l_den = (mu_x * mu_x + mu_r * mu_r)
        c_num = 2 * cov
        c_den = (var_x + var_r)
        ssim_map = (l_num * c_num) / (l_den * c_den + eps)               # (B,1,H,W)
        ssim_avg = ssim_map.mean(dim=(1, 2, 3))                          # (B,)
        return 1.0 - ssim_avg

    @torch.no_grad()
    def _s1_to_ref(self, Xb: Tensor, ref: Tensor) -> Tensor:
        dXdx = Xb[:, :, 1:] - Xb[:, :, :-1]
        dXdy = Xb[:, 1:, :] - Xb[:, :-1, :]
        dRdx = ref[:, 1:] - ref[:, :-1]
        dRdy = ref[1:, :] - ref[:-1, :]
        num_dx = (torch.abs(dXdx - dRdx.view(1, *dRdx.shape))).sum(dim=(1, 2))
        num_dy = (torch.abs(dXdy - dRdy.view(1, *dRdy.shape))).sum(dim=(1, 2))
        den_dx = torch.maximum(torch.abs(dXdx), torch.abs(dRdx).view(1, *dRdx.shape)).sum(dim=(1, 2))
        den_dy = torch.maximum(torch.abs(dXdy), torch.abs(dRdy).view(1, *dRdy.shape)).sum(dim=(1, 2))
        s1 = 100.0 * (num_dx + num_dy) / (den_dx + den_dy + 1e-12)
        return s1

    @torch.no_grad()
    def _distance_to_ref(self, Xb: Tensor, ref: Tensor) -> Tensor:
        """
        現在のactivation_distanceに対応した「Xb vs 単一参照ref」の距離ベクトル(B,)
        """
        if self.activation_distance == 'euclidean':
            return self._euclidean_to_ref(Xb, ref)
        elif self.activation_distance == 'ssim':
            return self._ssim_global_to_ref(Xb, ref)
        elif self.activation_distance == 'ssim5':
            return self._ssim5_to_ref(Xb, ref)
        elif self.activation_distance == 's1':
            return self._s1_to_ref(Xb, ref)
        elif self.activation_distance == 's1ssim':
            d1 = self._s1_to_ref(Xb, ref)
            d2 = self._ssim5_to_ref(Xb, ref)
            min1, _ = d1.min(dim=0, keepdim=True)
            max1, _ = d1.max(dim=0, keepdim=True)
            min2, _ = d2.min(dim=0, keepdim=True)
            max2, _ = d2.max(dim=0, keepdim=True)
            dn1 = (d1 - min1) / (max1 - min1 + 1e-12)
            dn2 = (d2 - min2) / (max2 - min2 + 1e-12)
            return 0.5 * (dn1 + dn2)
        else:
            raise RuntimeError('Unknown activation_distance')

    # ---------- 学習 ----------
    @torch.no_grad()
    def train_batch(self,
                    data: np.ndarray,
                    num_iteration: int,
                    batch_size: int = 32,
                    verbose: bool = True,
                    log_interval: int = 50,
                    update_per_iteration: bool = False,
                    shuffle: bool = True):
        """
        σは self.total_iters を基準に self.global_iter + it で一方向に減衰。
        複数回に分けて呼んでも、set_total_iterations(total) 済みなら継続減衰します。
        """
        N, D = data.shape
        H, W = self.field_shape
        if D != H * W:
            raise ValueError(f'data dimension {D} != H*W {H*W}')
        Xall = _to_device(data, self.device, self.dtype).reshape(N, H, W)

        qhist: List[float] = []
        rng_idx = torch.arange(N, device=self.device)

        # total iters 未設定なら今回のnum_iterationを総回数とみなす
        if self.total_iters is None:
            self.total_iters = int(num_iteration)

        iterator = trange(num_iteration) if verbose else range(num_iteration)
        for it in iterator:
            # 学習全体での反復数に基づくσ
            sigma = self._sigma_at_val(self.global_iter + it, self.total_iters)

            numerator = torch.zeros_like(self.weights)  # (m,H,W)
            denominator = torch.zeros((self.m,), device=self.device, dtype=self.dtype)

            if shuffle:
                perm = torch.randperm(N, device=self.device)
                idx_all = rng_idx[perm]
            else:
                idx_all = rng_idx

            for start in range(0, N, batch_size):
                end = min(start + batch_size, N)
                batch_idx = idx_all[start:end]
                Xb = Xall[batch_idx]

                bmu = self.bmu_indices(Xb, nodes_chunk=self.nodes_chunk)    # (B,)
                h = self._neighborhood(bmu, sigma)                          # (B,m)
                numerator += (h.unsqueeze(-1).unsqueeze(-1) * Xb.unsqueeze(1)).sum(dim=0)
                denominator += h.sum(dim=0)

                if update_per_iteration:
                    mask = (denominator > 0)
                    denom_safe = denominator.clone()
                    denom_safe[~mask] = 1.0
                    new_w = numerator / denom_safe.view(-1, 1, 1)
                    self.weights[mask] = new_w[mask]
                    numerator.zero_(); denominator.zero_()

            # 1イテレーションの最後に一括更新
            mask = (denominator > 0)
            if mask.any():
                denom_safe = denominator.clone()
                denom_safe[~mask] = 1.0
                new_w = numerator / denom_safe.view(-1, 1, 1)
                self.weights[mask] = new_w[mask]

            # 任意頻度のメドイド置換（距離一貫性の改善）
            if (self.medoid_replace_every is not None) and (((self.global_iter + it + 1) % self.medoid_replace_every) == 0):
                # 全データでBMUを計算して各ノードの最近傍サンプルで置換
                bmu_all = self.bmu_indices(Xall, nodes_chunk=self.nodes_chunk)  # (N,)
                for node in range(self.m):
                    idxs = (bmu_all == node).nonzero(as_tuple=False).flatten()
                    if idxs.numel() == 0:
                        continue
                    Xn = Xall[idxs]                                   # (Bn,H,W)
                    ref = self.weights[node]                          # (H,W)
                    d = self._distance_to_ref(Xn, ref)                # (Bn,)
                    pos = int(torch.argmin(d).item())
                    self.weights[node] = Xn[pos]

            # ログ用QE（固定サブセットを外部から渡すのが推奨だが、API互換のためここはそのまま）
            if (it % log_interval == 0) or (it == num_iteration - 1):
                qe = self.quantization_error(data, sample_limit=2048)
                qhist.append(qe)
                if verbose and hasattr(iterator, 'set_postfix'):
                    iterator.set_postfix(q_error=f"{qe:.6f}", sigma=f"{sigma:.3f}")

        # グローバル反復を進める
        self.global_iter += num_iteration

        return qhist

    # ---------- 評価 ----------
    @torch.no_grad()
    def quantization_error(self, data: np.ndarray, sample_limit: Optional[int] = None, batch_size: int = 64) -> float:
        N, D = data.shape
        H, W = self.field_shape
        if self.eval_indices is not None:
            # 固定評価
            X = _to_device(data, self.device, self.dtype)[self.eval_indices].reshape(-1, H, W)
        else:
            if sample_limit is not None and sample_limit < N:
                idx = np.random.choice(N, sample_limit, replace=False)
                X = data[idx]
            else:
                X = data
            X = _to_device(X, self.device, self.dtype).reshape(-1, H, W)
        total = 0.0; cnt = 0
        for start in range(0, X.shape[0], batch_size):
            end = min(start + batch_size, X.shape[0])
            Xb = X[start:end]
            d = self._distance_batch(Xb, nodes_chunk=self.nodes_chunk)
            mins = torch.min(d, dim=1).values
            total += float(mins.sum().item())
            cnt += Xb.shape[0]
        return total / max(1, cnt)

    @torch.no_grad()
    def predict(self, data: np.ndarray, batch_size: int = 64) -> np.ndarray:
        N, D = data.shape
        H, W = self.field_shape
        X = _to_device(data, self.device, self.dtype).reshape(N, H, W)
        bmu_all = []
        for start in range(0, N, batch_size):
            end = min(start + batch_size, N)
            Xb = X[start:end]
            bmu = self.bmu_indices(Xb, nodes_chunk=self.nodes_chunk)
            bmu_all.append(bmu)
        bmu_flat = torch.cat(bmu_all, dim=0)
        y = (bmu_flat % self.y).to(torch.long)
        x = (bmu_flat // self.y).to(torch.long)
        out = torch.stack([x, y], dim=1)
        return _as_numpy(out)

```

上記プログラムの結果が以下になります
上記と以下の内容も完璧に理解して記憶しておいてください

src/PressurePattern/use_result/results_v4_1000/learning_result/euclidean_som/euclidean_results.log
```
【SOM代表ノード群ベースの再現率（基本/複合）】
【各ラベルの再現率（代表ノード群ベース）】
 - 1  : N= 334 Correct= 302 Recall=0.9042 代表=[(3,1), (4,0), (4,1), (5,0), (5,1), (5,2), (6,0), (6,1), (6,2), (7,0), (7,1), (7,2), (7,3), (8,0), (8,1), (8,2), (8,3), (8,4), (9,0), (9,1), (9,2), (9,3)]
 - 2A : N=  49 Correct=  13 Recall=0.2653 代表=[(6,6), (6,7), (7,6)]
 - 2B : N=  74 Correct=  10 Recall=0.1351 代表=[(0,8), (5,5)]
 - 2C : N=  73 Correct=   7 Recall=0.0959 代表=[(0,9), (1,5)]
 - 2D : N= 151 Correct=  71 Recall=0.4702 代表=[(0,0), (0,1), (0,7), (1,0), (1,7), (2,0), (2,1), (3,0), (3,9), (7,4), (7,5), (7,8), (8,5)]
 - 3A : N=  49 Correct=  10 Recall=0.2041 代表=[(0,4), (2,7)]
 - 3B : N= 302 Correct= 225 Recall=0.7450 代表=[(1,2), (1,3), (2,2), (2,3), (2,4), (2,5), (2,9), (3,2), (3,3), (3,4), (3,5), (3,7), (4,2), (4,3), (4,4), (4,6), (5,3), (5,4), (6,3), (6,4), (8,6), (9,4), (9,5), (9,6), (9,7), (9,8), (9,9)]
 - 3C : N=  84 Correct=   3 Recall=0.0357 代表=[(1,1)]
 - 3D : N=  55 Correct=   3 Recall=0.0545 代表=[(4,5)]
 - 4A : N= 193 Correct= 115 Recall=0.5959 代表=[(0,5), (0,6), (1,4), (1,8), (1,9), (2,8), (3,6), (4,7), (4,8), (4,9), (5,6), (5,9), (6,5), (6,8), (6,9), (7,7), (7,9), (8,9)]
 - 4B : N= 113 Correct=  20 Recall=0.1770 代表=[(0,2), (0,3), (1,6), (3,8), (5,7), (5,8), (8,8)]
 - 5  : N=  60 Correct=   8 Recall=0.1333 代表=[(2,6), (8,7)]
 - 6A : N=   7 Correct=   0 Recall=0.0000 代表=なし
 - 6B : N=  13 Correct=   0 Recall=0.0000 代表=なし
 - 6C : N=   2 Correct=   0 Recall=0.0000 代表=なし

【複合ラベル考慮の再現率（基本+応用）】
 - 1  : N= 682 Correct= 541 Recall=0.7933
 - 2A : N= 596 Correct=  54 Recall=0.0906
 - 2B : N= 250 Correct=  18 Recall=0.0720
 - 2C : N= 320 Correct=  15 Recall=0.0469
 - 2D : N= 232 Correct=  90 Recall=0.3879
 - 3A : N= 312 Correct=  25 Recall=0.0801
 - 3B : N= 588 Correct= 367 Recall=0.6241
 - 3C : N= 224 Correct=   6 Recall=0.0268
 - 3D : N= 166 Correct=   3 Recall=0.0181
 - 4A : N= 542 Correct= 255 Recall=0.4705
 - 4B : N= 636 Correct=  63 Recall=0.0991
 - 5  : N= 269 Correct=  23 Recall=0.0855
 - 6A : N= 137 Correct=   0 Recall=0.0000
 - 6B : N=  58 Correct=   0 Recall=0.0000
 - 6C : N=   3 Correct=   0 Recall=0.0000

[Summary] Macro Recall (基本ラベル)   = 0.2544
[Summary] Macro Recall (基本+応用) = 0.1863
```

src/PressurePattern/use_result/results_v4_1000/verification_results/euclidean_som/euclidean_verification.log
```
【各ラベルの再現率（学習時の代表基本ラベルを予測として）】
 - 1  : N_base=  41 Correct_base=  36 Recall_base=0.8780 | N_comp=  78 Correct_comp=  62 Recall_comp=0.7949
 - 2A : N_base=  12 Correct_base=   2 Recall_base=0.1667 | N_comp=  73 Correct_comp=   8 Recall_comp=0.1096
 - 2B : N_base=   7 Correct_base=   0 Recall_base=0.0000 | N_comp=  27 Correct_comp=   0 Recall_comp=0.0000
 - 2C : N_base=   7 Correct_base=   0 Recall_base=0.0000 | N_comp=  25 Correct_comp=   2 Recall_comp=0.0800
 - 2D : N_base=  19 Correct_base=   4 Recall_base=0.2105 | N_comp=  30 Correct_comp=   5 Recall_comp=0.1667
 - 3A : N_base=   4 Correct_base=   0 Recall_base=0.0000 | N_comp=  21 Correct_comp=   1 Recall_comp=0.0476
 - 3B : N_base=  19 Correct_base=  14 Recall_base=0.7368 | N_comp=  58 Correct_comp=  33 Recall_comp=0.5690
 - 3C : N_base=  17 Correct_base=   0 Recall_base=0.0000 | N_comp=  29 Correct_comp=   1 Recall_comp=0.0345
 - 3D : N_base=   8 Correct_base=   0 Recall_base=0.0000 | N_comp=  20 Correct_comp=   0 Recall_comp=0.0000
 - 4A : N_base=  10 Correct_base=   4 Recall_base=0.4000 | N_comp=  57 Correct_comp=  24 Recall_comp=0.4211
 - 4B : N_base=  13 Correct_base=   2 Recall_base=0.1538 | N_comp=  57 Correct_comp=   5 Recall_comp=0.0877
 - 5  : N_base=   9 Correct_base=   0 Recall_base=0.0000 | N_comp=  49 Correct_comp=   4 Recall_comp=0.0816
 - 6A : N_base=   0 Correct_base=   0 Recall_base=0.0000 | N_comp=  28 Correct_comp=   0 Recall_comp=0.0000
 - 6B : N_base=   2 Correct_base=   0 Recall_base=0.0000 | N_comp=  10 Correct_comp=   0 Recall_comp=0.0000
 - 6C : N_base=   0 Correct_base=   0 Recall_base=0.0000 | N_comp=   2 Correct_comp=   0 Recall_comp=0.0000

[Summary] Macro Recall (基本ラベル)   = 0.1958
[Summary] Macro Recall (基本+応用) = 0.1595
```

src/PressurePattern/use_result/results_v4_1000/learning_result/s1_som/s1_results.log
```
【SOM代表ノード群ベースの再現率（基本/複合）】
【各ラベルの再現率（代表ノード群ベース）】
 - 1  : N= 334 Correct= 311 Recall=0.9311 代表=[(2,1), (2,2), (3,1), (3,2), (3,3), (4,1), (4,2), (4,3), (5,1), (5,2), (5,3), (6,2)]
 - 2A : N=  49 Correct=  21 Recall=0.4286 代表=[(0,2), (1,2), (1,3), (2,3)]
 - 2B : N=  74 Correct=  16 Recall=0.2162 代表=[(6,4), (6,6), (7,5)]
 - 2C : N=  73 Correct=  13 Recall=0.1781 代表=[(4,5), (4,8), (7,6), (8,6)]
 - 2D : N= 151 Correct=  79 Recall=0.5232 代表=[(1,4), (1,5), (2,4), (2,5), (2,6), (3,4), (3,5), (3,6), (4,4), (5,4), (5,8), (6,5), (7,4), (8,5)]
 - 3A : N=  49 Correct=  14 Recall=0.2857 代表=[(3,9), (9,0)]
 - 3B : N= 302 Correct= 237 Recall=0.7848 代表=[(1,0), (1,1), (2,0), (3,0), (4,0), (4,9), (5,0), (5,9), (6,0), (6,1), (7,0), (7,1), (7,3), (8,2), (8,3), (9,2), (9,3), (9,4), (9,5)]
 - 3C : N=  84 Correct=   2 Recall=0.0238 代表=[(8,4)]
 - 3D : N=  55 Correct=   0 Recall=0.0000 代表=なし
 - 4A : N= 193 Correct= 133 Recall=0.6891 代表=[(0,0), (0,1), (0,4), (0,5), (0,6), (0,7), (0,9), (1,6), (1,7), (2,8), (6,3), (6,7), (6,8), (6,9), (7,7), (7,8), (8,7), (8,8), (8,9), (9,7), (9,8)]
 - 4B : N= 113 Correct=  33 Recall=0.2920 代表=[(0,3), (0,8), (3,8), (4,6), (4,7), (5,5), (5,6), (7,2), (8,0), (8,1), (9,1), (9,6)]
 - 5  : N=  60 Correct=  29 Recall=0.4833 代表=[(2,9), (7,9), (9,9)]
 - 6A : N=   7 Correct=   2 Recall=0.2857 代表=[(1,9)]
 - 6B : N=  13 Correct=   1 Recall=0.0769 代表=[(2,7)]
 - 6C : N=   2 Correct=   0 Recall=0.0000 代表=なし

【複合ラベル考慮の再現率（基本+応用）】
 - 1  : N= 682 Correct= 548 Recall=0.8035
 - 2A : N= 596 Correct= 106 Recall=0.1779
 - 2B : N= 250 Correct=  26 Recall=0.1040
 - 2C : N= 320 Correct=  21 Recall=0.0656
 - 2D : N= 232 Correct= 103 Recall=0.4440
 - 3A : N= 312 Correct=  27 Recall=0.0865
 - 3B : N= 588 Correct= 378 Recall=0.6429
 - 3C : N= 224 Correct=   5 Recall=0.0223
 - 3D : N= 166 Correct=   0 Recall=0.0000
 - 4A : N= 542 Correct= 285 Recall=0.5258
 - 4B : N= 636 Correct=  93 Recall=0.1462
 - 5  : N= 269 Correct=  95 Recall=0.3532
 - 6A : N= 137 Correct=  12 Recall=0.0876
 - 6B : N=  58 Correct=   2 Recall=0.0345
 - 6C : N=   3 Correct=   0 Recall=0.0000

[Summary] Macro Recall (基本ラベル)   = 0.3466
[Summary] Macro Recall (基本+応用) = 0.2329
```

src/PressurePattern/use_result/results_v4_1000/verification_results/s1_som/s1_verification.log
```
【各ラベルの再現率（学習時の代表基本ラベルを予測として）】
 - 1  : N_base=  41 Correct_base=  36 Recall_base=0.8780 | N_comp=  78 Correct_comp=  60 Recall_comp=0.7692
 - 2A : N_base=  12 Correct_base=   3 Recall_base=0.2500 | N_comp=  73 Correct_comp=  12 Recall_comp=0.1644
 - 2B : N_base=   7 Correct_base=   1 Recall_base=0.1429 | N_comp=  27 Correct_comp=   1 Recall_comp=0.0370
 - 2C : N_base=   7 Correct_base=   0 Recall_base=0.0000 | N_comp=  25 Correct_comp=   1 Recall_comp=0.0400
 - 2D : N_base=  19 Correct_base=   7 Recall_base=0.3684 | N_comp=  30 Correct_comp=   8 Recall_comp=0.2667
 - 3A : N_base=   4 Correct_base=   0 Recall_base=0.0000 | N_comp=  21 Correct_comp=   0 Recall_comp=0.0000
 - 3B : N_base=  19 Correct_base=  16 Recall_base=0.8421 | N_comp=  58 Correct_comp=  35 Recall_comp=0.6034
 - 3C : N_base=  17 Correct_base=   0 Recall_base=0.0000 | N_comp=  29 Correct_comp=   0 Recall_comp=0.0000
 - 3D : N_base=   8 Correct_base=   0 Recall_base=0.0000 | N_comp=  20 Correct_comp=   0 Recall_comp=0.0000
 - 4A : N_base=  10 Correct_base=   8 Recall_base=0.8000 | N_comp=  57 Correct_comp=  22 Recall_comp=0.3860
 - 4B : N_base=  13 Correct_base=   1 Recall_base=0.0769 | N_comp=  57 Correct_comp=   6 Recall_comp=0.1053
 - 5  : N_base=   9 Correct_base=   2 Recall_base=0.2222 | N_comp=  49 Correct_comp=  18 Recall_comp=0.3673
 - 6A : N_base=   0 Correct_base=   0 Recall_base=0.0000 | N_comp=  28 Correct_comp=   1 Recall_comp=0.0357
 - 6B : N_base=   2 Correct_base=   0 Recall_base=0.0000 | N_comp=  10 Correct_comp=   0 Recall_comp=0.0000
 - 6C : N_base=   0 Correct_base=   0 Recall_base=0.0000 | N_comp=   2 Correct_comp=   0 Recall_comp=0.0000

[Summary] Macro Recall (基本ラベル)   = 0.2754
[Summary] Macro Recall (基本+応用) = 0.1850
```

src/PressurePattern/use_result/results_v4_1000/learning_result/s1ssim_som/s1ssim_results.log
```
【SOM代表ノード群ベースの再現率（基本/複合）】
【各ラベルの再現率（代表ノード群ベース）】
 - 1  : N= 334 Correct= 299 Recall=0.8952 代表=[(1,0), (2,0), (2,1), (2,2), (3,0), (3,1), (4,0), (4,1), (5,0), (5,1), (6,0), (6,1)]
 - 2A : N=  49 Correct=  22 Recall=0.4490 代表=[(0,4), (0,5), (0,6), (0,7), (1,1)]
 - 2B : N=  74 Correct=  10 Recall=0.1351 代表=[(0,3), (0,8)]
 - 2C : N=  73 Correct=  11 Recall=0.1507 代表=[(5,6), (7,4), (7,6), (9,1)]
 - 2D : N= 151 Correct=  75 Recall=0.4967 代表=[(0,0), (0,1), (0,2), (0,9), (1,2), (6,6), (7,5), (7,9), (8,0), (9,0)]
 - 3A : N=  49 Correct=  15 Recall=0.3061 代表=[(6,2), (7,3), (8,3), (9,4)]
 - 3B : N= 302 Correct= 228 Recall=0.7550 代表=[(2,3), (2,4), (2,5), (3,2), (3,3), (3,4), (3,5), (4,2), (4,3), (4,4), (5,2), (5,3), (5,4), (6,3), (6,9), (7,0)]
 - 3C : N=  84 Correct=   1 Recall=0.0119 代表=[(1,3)]
 - 3D : N=  55 Correct=   0 Recall=0.0000 代表=なし
 - 4A : N= 193 Correct= 130 Recall=0.6736 代表=[(1,4), (1,7), (1,8), (2,7), (2,8), (2,9), (3,7), (3,8), (3,9), (4,8), (4,9), (5,5), (5,7), (5,8), (5,9), (6,4), (6,5), (6,7), (6,8), (7,2), (8,9), (9,3), (9,7), (9,8), (9,9)]
 - 4B : N= 113 Correct=  28 Recall=0.2478 代表=[(1,5), (1,9), (4,7), (7,1), (7,7), (7,8), (8,1), (8,2), (9,2)]
 - 5  : N=  60 Correct=  31 Recall=0.5167 代表=[(1,6), (2,6), (3,6), (4,5), (4,6), (8,5), (8,6)]
 - 6A : N=   7 Correct=   1 Recall=0.1429 代表=[(9,5)]
 - 6B : N=  13 Correct=   5 Recall=0.3846 代表=[(8,7), (8,8), (9,6)]
 - 6C : N=   2 Correct=   0 Recall=0.0000 代表=なし

【複合ラベル考慮の再現率（基本+応用）】
 - 1  : N= 682 Correct= 523 Recall=0.7669
 - 2A : N= 596 Correct= 111 Recall=0.1862
 - 2B : N= 250 Correct=  15 Recall=0.0600
 - 2C : N= 320 Correct=  21 Recall=0.0656
 - 2D : N= 232 Correct= 101 Recall=0.4353
 - 3A : N= 312 Correct=  43 Recall=0.1378
 - 3B : N= 588 Correct= 360 Recall=0.6122
 - 3C : N= 224 Correct=   5 Recall=0.0223
 - 3D : N= 166 Correct=   0 Recall=0.0000
 - 4A : N= 542 Correct= 293 Recall=0.5406
 - 4B : N= 636 Correct=  78 Recall=0.1226
 - 5  : N= 269 Correct=  98 Recall=0.3643
 - 6A : N= 137 Correct=  10 Recall=0.0730
 - 6B : N=  58 Correct=   9 Recall=0.1552
 - 6C : N=   3 Correct=   0 Recall=0.0000

[Summary] Macro Recall (基本ラベル)   = 0.3443
[Summary] Macro Recall (基本+応用) = 0.2361
```

src/PressurePattern/use_result/results_v4_1000/verification_results/s1ssim_som/s1ssim_verification.log
```
【各ラベルの再現率（学習時の代表基本ラベルを予測として）】
 - 1  : N_base=  41 Correct_base=  36 Recall_base=0.8780 | N_comp=  78 Correct_comp=  58 Recall_comp=0.7436
 - 2A : N_base=  12 Correct_base=   3 Recall_base=0.2500 | N_comp=  73 Correct_comp=  17 Recall_comp=0.2329
 - 2B : N_base=   7 Correct_base=   0 Recall_base=0.0000 | N_comp=  27 Correct_comp=   0 Recall_comp=0.0000
 - 2C : N_base=   7 Correct_base=   0 Recall_base=0.0000 | N_comp=  25 Correct_comp=   2 Recall_comp=0.0800
 - 2D : N_base=  19 Correct_base=   7 Recall_base=0.3684 | N_comp=  30 Correct_comp=   8 Recall_comp=0.2667
 - 3A : N_base=   4 Correct_base=   3 Recall_base=0.7500 | N_comp=  21 Correct_comp=   6 Recall_comp=0.2857
 - 3B : N_base=  19 Correct_base=  17 Recall_base=0.8947 | N_comp=  58 Correct_comp=  35 Recall_comp=0.6034
 - 3C : N_base=  17 Correct_base=   0 Recall_base=0.0000 | N_comp=  29 Correct_comp=   0 Recall_comp=0.0000
 - 3D : N_base=   8 Correct_base=   0 Recall_base=0.0000 | N_comp=  20 Correct_comp=   0 Recall_comp=0.0000
 - 4A : N_base=  10 Correct_base=   5 Recall_base=0.5000 | N_comp=  57 Correct_comp=  23 Recall_comp=0.4035
 - 4B : N_base=  13 Correct_base=   1 Recall_base=0.0769 | N_comp=  57 Correct_comp=   2 Recall_comp=0.0351
 - 5  : N_base=   9 Correct_base=   3 Recall_base=0.3333 | N_comp=  49 Correct_comp=  15 Recall_comp=0.3061
 - 6A : N_base=   0 Correct_base=   0 Recall_base=0.0000 | N_comp=  28 Correct_comp=   5 Recall_comp=0.1786
 - 6B : N_base=   2 Correct_base=   1 Recall_base=0.5000 | N_comp=  10 Correct_comp=   1 Recall_comp=0.1000
 - 6C : N_base=   0 Correct_base=   0 Recall_base=0.0000 | N_comp=   2 Correct_comp=   0 Recall_comp=0.0000

[Summary] Macro Recall (基本ラベル)   = 0.3501
[Summary] Macro Recall (基本+応用) = 0.2157
```

src/PressurePattern/use_result/results_v4_1000/learning_result/ssim_som/ssim_results.log
```
【SOM代表ノード群ベースの再現率（基本/複合）】
【各ラベルの再現率（代表ノード群ベース）】
 - 1  : N= 334 Correct= 294 Recall=0.8802 代表=[(0,4), (0,5), (0,6), (0,7), (0,8), (0,9), (1,4), (1,5), (1,6), (1,7), (1,8), (1,9), (2,4), (2,5), (2,8)]
 - 2A : N=  49 Correct=  12 Recall=0.2449 代表=[(4,5), (4,7), (5,7)]
 - 2B : N=  74 Correct=   4 Recall=0.0541 代表=[(7,2), (8,3)]
 - 2C : N=  73 Correct=   7 Recall=0.0959 代表=[(1,0), (7,3), (8,2)]
 - 2D : N= 151 Correct=  63 Recall=0.4172 代表=[(0,2), (0,3), (1,3), (2,0), (2,1), (3,7), (6,8), (8,5), (9,0), (9,1), (9,3), (9,4)]
 - 3A : N=  49 Correct=  13 Recall=0.2653 代表=[(0,0), (6,0)]
 - 3B : N= 302 Correct= 184 Recall=0.6093 代表=[(0,1), (1,2), (2,2), (2,3), (2,6), (2,7), (2,9), (3,1), (3,2), (3,3), (3,4), (3,5), (3,6), (3,9), (4,0), (4,1), (4,2), (4,3), (4,4), (4,9), (5,2), (5,3), (5,9), (6,2), (6,4), (6,9), (9,8)]
 - 3C : N=  84 Correct=  11 Recall=0.1310 代表=[(3,0), (3,8), (7,4)]
 - 3D : N=  55 Correct=   0 Recall=0.0000 代表=なし
 - 4A : N= 193 Correct= 113 Recall=0.5855 代表=[(4,6), (5,1), (5,4), (5,6), (6,1), (6,3), (6,5), (6,6), (7,0), (7,6), (7,7), (7,8), (7,9), (8,1), (8,4), (8,6), (8,7), (8,9), (9,2), (9,5), (9,6), (9,7), (9,9)]
 - 4B : N= 113 Correct=  25 Recall=0.2212 代表=[(1,1), (4,8), (5,0), (5,8), (6,7), (7,1), (8,0), (8,8)]
 - 5  : N=  60 Correct=  11 Recall=0.1833 代表=[(5,5), (7,5)]
 - 6A : N=   7 Correct=   0 Recall=0.0000 代表=なし
 - 6B : N=  13 Correct=   0 Recall=0.0000 代表=なし
 - 6C : N=   2 Correct=   0 Recall=0.0000 代表=なし

【複合ラベル考慮の再現率（基本+応用）】
 - 1  : N= 682 Correct= 559 Recall=0.8196
 - 2A : N= 596 Correct=  52 Recall=0.0872
 - 2B : N= 250 Correct=   7 Recall=0.0280
 - 2C : N= 320 Correct=  17 Recall=0.0531
 - 2D : N= 232 Correct=  85 Recall=0.3664
 - 3A : N= 312 Correct=  46 Recall=0.1474
 - 3B : N= 588 Correct= 300 Recall=0.5102
 - 3C : N= 224 Correct=  23 Recall=0.1027
 - 3D : N= 166 Correct=   0 Recall=0.0000
 - 4A : N= 542 Correct= 254 Recall=0.4686
 - 4B : N= 636 Correct=  52 Recall=0.0818
 - 5  : N= 269 Correct=  17 Recall=0.0632
 - 6A : N= 137 Correct=   0 Recall=0.0000
 - 6B : N=  58 Correct=   0 Recall=0.0000
 - 6C : N=   3 Correct=   0 Recall=0.0000

[Summary] Macro Recall (基本ラベル)   = 0.2459
[Summary] Macro Recall (基本+応用) = 0.1819
```

src/PressurePattern/use_result/results_v4_1000/verification_results/ssim_som/ssim_verification.log
```
【各ラベルの再現率（学習時の代表基本ラベルを予測として）】
 - 1  : N_base=  41 Correct_base=  39 Recall_base=0.9512 | N_comp=  78 Correct_comp=  69 Recall_comp=0.8846
 - 2A : N_base=  12 Correct_base=   2 Recall_base=0.1667 | N_comp=  73 Correct_comp=   8 Recall_comp=0.1096
 - 2B : N_base=   7 Correct_base=   1 Recall_base=0.1429 | N_comp=  27 Correct_comp=   1 Recall_comp=0.0370
 - 2C : N_base=   7 Correct_base=   0 Recall_base=0.0000 | N_comp=  25 Correct_comp=   1 Recall_comp=0.0400
 - 2D : N_base=  19 Correct_base=   5 Recall_base=0.2632 | N_comp=  30 Correct_comp=   8 Recall_comp=0.2667
 - 3A : N_base=   4 Correct_base=   0 Recall_base=0.0000 | N_comp=  21 Correct_comp=   2 Recall_comp=0.0952
 - 3B : N_base=  19 Correct_base=   8 Recall_base=0.4211 | N_comp=  58 Correct_comp=  23 Recall_comp=0.3966
 - 3C : N_base=  17 Correct_base=   0 Recall_base=0.0000 | N_comp=  29 Correct_comp=   1 Recall_comp=0.0345
 - 3D : N_base=   8 Correct_base=   0 Recall_base=0.0000 | N_comp=  20 Correct_comp=   0 Recall_comp=0.0000
 - 4A : N_base=  10 Correct_base=   6 Recall_base=0.6000 | N_comp=  57 Correct_comp=  16 Recall_comp=0.2807
 - 4B : N_base=  13 Correct_base=   3 Recall_base=0.2308 | N_comp=  57 Correct_comp=   6 Recall_comp=0.1053
 - 5  : N_base=   9 Correct_base=   0 Recall_base=0.0000 | N_comp=  49 Correct_comp=   0 Recall_comp=0.0000
 - 6A : N_base=   0 Correct_base=   0 Recall_base=0.0000 | N_comp=  28 Correct_comp=   0 Recall_comp=0.0000
 - 6B : N_base=   2 Correct_base=   0 Recall_base=0.0000 | N_comp=  10 Correct_comp=   0 Recall_comp=0.0000
 - 6C : N_base=   0 Correct_base=   0 Recall_base=0.0000 | N_comp=   2 Correct_comp=   0 Recall_comp=0.0000

[Summary] Macro Recall (基本ラベル)   = 0.2135
[Summary] Macro Recall (基本+応用) = 0.1500
```

src/PressurePattern/use_result/results_v4_1000/learning_result/ssim5_som/ssim5_results.log
```
【SOM代表ノード群ベースの再現率（基本/複合）】
【各ラベルの再現率（代表ノード群ベース）】
 - 1  : N= 334 Correct= 299 Recall=0.8952 代表=[(0,6), (0,7), (1,6), (2,5), (8,5), (8,6), (8,7), (8,8), (8,9), (9,5), (9,6), (9,7), (9,8), (9,9)]
 - 2A : N=  49 Correct=  13 Recall=0.2653 代表=[(5,3), (5,4), (6,3), (6,4), (7,8)]
 - 2B : N=  74 Correct=   4 Recall=0.0541 代表=[(0,2), (6,6)]
 - 2C : N=  73 Correct=  12 Recall=0.1644 代表=[(2,4), (5,9), (9,0)]
 - 2D : N= 151 Correct=  67 Recall=0.4437 代表=[(0,3), (0,4), (0,5), (1,2), (1,3), (1,4), (1,5), (4,3), (4,4), (6,8), (6,9), (7,7), (7,9), (8,1)]
 - 3A : N=  49 Correct=  14 Recall=0.2857 代表=[(0,1), (2,7), (3,9), (4,7)]
 - 3B : N= 302 Correct= 227 Recall=0.7517 代表=[(0,0), (0,8), (0,9), (1,0), (1,7), (1,8), (1,9), (2,8), (2,9), (3,7), (3,8), (4,2), (4,8), (6,7), (7,5), (7,6), (9,3), (9,4)]
 - 3C : N=  84 Correct=   0 Recall=0.0000 代表=なし
 - 3D : N=  55 Correct=   7 Recall=0.1273 代表=[(8,4)]
 - 4A : N= 193 Correct= 126 Recall=0.6528 代表=[(1,1), (2,0), (2,1), (2,2), (3,0), (3,1), (4,0), (4,1), (4,5), (4,9), (5,0), (5,1), (5,8), (6,0), (6,2), (6,5), (7,0), (7,1), (7,2), (7,3), (8,0), (8,3), (9,2)]
 - 4B : N= 113 Correct=  26 Recall=0.2301 代表=[(2,3), (2,6), (3,2), (3,3), (3,4), (3,6), (5,2), (6,1)]
 - 5  : N=  60 Correct=  17 Recall=0.2833 代表=[(5,6), (7,4), (8,2), (9,1)]
 - 6A : N=   7 Correct=   2 Recall=0.2857 代表=[(4,6)]
 - 6B : N=  13 Correct=   0 Recall=0.0000 代表=なし
 - 6C : N=   2 Correct=   0 Recall=0.0000 代表=なし

【複合ラベル考慮の再現率（基本+応用）】
 - 1  : N= 682 Correct= 544 Recall=0.7977
 - 2A : N= 596 Correct=  60 Recall=0.1007
 - 2B : N= 250 Correct=   9 Recall=0.0360
 - 2C : N= 320 Correct=  23 Recall=0.0719
 - 2D : N= 232 Correct=  82 Recall=0.3534
 - 3A : N= 312 Correct=  42 Recall=0.1346
 - 3B : N= 588 Correct= 363 Recall=0.6173
 - 3C : N= 224 Correct=   0 Recall=0.0000
 - 3D : N= 166 Correct=   9 Recall=0.0542
 - 4A : N= 542 Correct= 300 Recall=0.5535
 - 4B : N= 636 Correct=  59 Recall=0.0928
 - 5  : N= 269 Correct=  51 Recall=0.1896
 - 6A : N= 137 Correct=  12 Recall=0.0876
 - 6B : N=  58 Correct=   0 Recall=0.0000
 - 6C : N=   3 Correct=   0 Recall=0.0000

[Summary] Macro Recall (基本ラベル)   = 0.2960
[Summary] Macro Recall (基本+応用) = 0.2060
```

src/PressurePattern/use_result/results_v4_1000/verification_results/ssim5_som/ssim5_verification.log
```
【各ラベルの再現率（学習時の代表基本ラベルを予測として）】
 - 1  : N_base=  41 Correct_base=  36 Recall_base=0.8780 | N_comp=  78 Correct_comp=  62 Recall_comp=0.7949
 - 2A : N_base=  12 Correct_base=   1 Recall_base=0.0833 | N_comp=  73 Correct_comp=   8 Recall_comp=0.1096
 - 2B : N_base=   7 Correct_base=   0 Recall_base=0.0000 | N_comp=  27 Correct_comp=   0 Recall_comp=0.0000
 - 2C : N_base=   7 Correct_base=   0 Recall_base=0.0000 | N_comp=  25 Correct_comp=   1 Recall_comp=0.0400
 - 2D : N_base=  19 Correct_base=  10 Recall_base=0.5263 | N_comp=  30 Correct_comp=  11 Recall_comp=0.3667
 - 3A : N_base=   4 Correct_base=   1 Recall_base=0.2500 | N_comp=  21 Correct_comp=   4 Recall_comp=0.1905
 - 3B : N_base=  19 Correct_base=  16 Recall_base=0.8421 | N_comp=  58 Correct_comp=  37 Recall_comp=0.6379
 - 3C : N_base=  17 Correct_base=   0 Recall_base=0.0000 | N_comp=  29 Correct_comp=   0 Recall_comp=0.0000
 - 3D : N_base=   8 Correct_base=   0 Recall_base=0.0000 | N_comp=  20 Correct_comp=   0 Recall_comp=0.0000
 - 4A : N_base=  10 Correct_base=   6 Recall_base=0.6000 | N_comp=  57 Correct_comp=  28 Recall_comp=0.4912
 - 4B : N_base=  13 Correct_base=   0 Recall_base=0.0000 | N_comp=  57 Correct_comp=   2 Recall_comp=0.0351
 - 5  : N_base=   9 Correct_base=   1 Recall_base=0.1111 | N_comp=  49 Correct_comp=   5 Recall_comp=0.1020
 - 6A : N_base=   0 Correct_base=   0 Recall_base=0.0000 | N_comp=  28 Correct_comp=   3 Recall_comp=0.1071
 - 6B : N_base=   2 Correct_base=   0 Recall_base=0.0000 | N_comp=  10 Correct_comp=   0 Recall_comp=0.0000
 - 6C : N_base=   0 Correct_base=   0 Recall_base=0.0000 | N_comp=   2 Correct_comp=   0 Recall_comp=0.0000

[Summary] Macro Recall (基本ラベル)   = 0.2531
[Summary] Macro Recall (基本+応用) = 0.1917
```